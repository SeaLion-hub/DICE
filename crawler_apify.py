# crawler_apify.py (ìµœì‹  ë°ì´í„° ê°€ì ¸ì˜¤ê¸° + ìµœì‹  AI ì²˜ë¦¬ ë°©ì‹ í†µí•©)
import os
import time
import json
import hashlib
import requests
import psycopg2
import re
import datetime as dt
from datetime import datetime, timezone
from urllib.parse import urlencode, urlparse, urljoin
from psycopg2.extras import RealDictCursor, Json
from dotenv import load_dotenv
from typing import Optional, Dict, Any, List, Tuple
from html import unescape
from bs4 import BeautifulSoup
from colleges import COLLEGES

# AI processor import ìˆ˜ì • (ìµœì‹  í•¨ìˆ˜ ì‚¬ìš©)
from ai_processor import (
    classify_notice_category,
    extract_structured_info,
)
# _to_utc_ts í•¨ìˆ˜ëŠ” main.py ë˜ëŠ” backfill_ai.py ë“± ë‹¤ë¥¸ ê³³ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ì—¬ê¸°ì— ì •ì˜í•´ì•¼ í•©ë‹ˆë‹¤.
# ì—¬ê¸°ì„œëŠ” backfill_ai.pyì˜ í•¨ìˆ˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
from backfill_ai import _to_utc_ts # _to_utc_ts ì„í¬íŠ¸ ë˜ëŠ” ì§ì ‘ ì •ì˜ í•„ìš”

load_dotenv(encoding="utf-8")

APIFY_TOKEN = os.getenv("APIFY_TOKEN")
DATABASE_URL = os.getenv("DATABASE_URL")
AI_IN_PIPELINE = os.getenv("AI_IN_PIPELINE", "true").lower() == "true"
AI_SLEEP_SEC = float(os.getenv("AI_SLEEP_SEC", "0.8"))
AI_MAX_PER_COLLEGE = int(os.getenv("AI_MAX_PER_COLLEGE", "999999")) # AI í˜¸ì¶œ ì œí•œ

if not APIFY_TOKEN:
    raise RuntimeError("APIFY_TOKEN not set")
if not DATABASE_URL:
    raise RuntimeError("DATABASE_URL not set")

SESSION = requests.Session()
SESSION.headers.update({"Accept": "application/json"})

# AI í•„ë“œ í¬í•¨ëœ UPSERT SQL (search_vector í¬í•¨í•˜ì—¬ main.pyì™€ ìœ ì‚¬í•˜ê²Œ)
UPSERT_SQL = """
INSERT INTO notices (
    college_key, title, url, summary_raw, body_html, body_text,
    published_at, source_site, content_hash,
    category_ai, start_at_ai, end_at_ai, qualification_ai, hashtags_ai,
    search_vector -- search_vector ì¶”ê°€
) VALUES (
    %(college_key)s, %(title)s, %(url)s, %(summary_raw)s,
    %(body_html)s, %(body_text)s, %(published_at)s,
    %(source_site)s, %(content_hash)s,
    %(category_ai)s, %(start_at_ai)s, %(end_at_ai)s, %(qualification_ai)s, %(hashtags_ai)s,
    setweight(to_tsvector('simple', coalesce(%(title)s, '')), 'A') ||
    setweight(to_tsvector('simple', coalesce(array_to_string(%(hashtags_ai)s, ' '), '')), 'B') ||
    setweight(to_tsvector('simple', coalesce(%(body_text)s, '')), 'C') -- search_vector ê°’ ìƒì„± ë¡œì§ ì¶”ê°€
)
ON CONFLICT (content_hash)
DO UPDATE SET
    title = EXCLUDED.title, -- title ë“± ë‹¤ë¥¸ í•„ë“œë„ ì—…ë°ì´íŠ¸ë˜ë„ë¡ ìˆ˜ì • (main.py ì°¸ê³ )
    url = EXCLUDED.url,
    summary_raw = EXCLUDED.summary_raw,
    body_html = EXCLUDED.body_html,
    body_text = EXCLUDED.body_text,
    published_at = EXCLUDED.published_at,
    category_ai = EXCLUDED.category_ai,
    start_at_ai = EXCLUDED.start_at_ai,
    end_at_ai = EXCLUDED.end_at_ai,
    qualification_ai = EXCLUDED.qualification_ai,
    hashtags_ai = EXCLUDED.hashtags_ai,
    updated_at = CURRENT_TIMESTAMP,
    search_vector = setweight(to_tsvector('simple', coalesce(EXCLUDED.title, '')), 'A') ||
                    setweight(to_tsvector('simple', coalesce(array_to_string(EXCLUDED.hashtags_ai, ' '), '')), 'B') ||
                    setweight(to_tsvector('simple', coalesce(EXCLUDED.body_text, '')), 'C') -- search_vector ì—…ë°ì´íŠ¸ ë¡œì§ ì¶”ê°€
"""

# --- ê¸°ì¡´ í—¬í¼ í•¨ìˆ˜ë“¤ (clean_text, extract_text_from_html ë“±) ---
# (ì´ì „ ì½”ë“œì™€ ë™ì¼í•˜ë¯€ë¡œ ìƒëµ, í•„ìš”ì‹œ ì´ì „ ì½”ë“œ ë¸”ë¡ì—ì„œ ë³µì‚¬)
def clean_text(text: Optional[str], max_length: Optional[int] = None) -> str:
    """í…ìŠ¤íŠ¸ ì •ë¦¬ ë° ì •ê·œí™”"""
    if not text:
        return ""
    text = unescape(text)
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    if max_length and len(text) > max_length:
        text = text[:max_length-3] + "..."
    return text

def extract_text_from_html(html: Optional[str]) -> str:
    """HTMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „ ìœ ì§€)"""
    if not html:
        return ""
    try:
        content_pattern = r'ê²Œì‹œê¸€ ë‚´ìš©(.*?)ëª©ë¡'
        content_match = re.search(content_pattern, html, re.DOTALL)
        if content_match:
            extracted_text = content_match.group(1).strip()
            soup = BeautifulSoup(extracted_text, 'html.parser')
            for tag in soup(['script', 'style', 'meta', 'link']):
                tag.decompose()
            text = soup.get_text(separator=' ', strip=True)
            return clean_text(text)
        soup = BeautifulSoup(html, 'html.parser')
        for tag in soup(['script', 'style', 'meta', 'link']):
            tag.decompose()
        text = soup.get_text(separator=' ', strip=True)
        return clean_text(text)
    except Exception as e:
        print(f"  âš ï¸ HTML parsing error: {e}")
        return ""

def normalize_url(url: Optional[str], base_url: Optional[str] = None) -> str:
    """URL ì •ê·œí™” ë° ì ˆëŒ€ ê²½ë¡œ ë³€í™˜"""
    if not url: return ""
    url = url.strip()
    if base_url and not url.startswith(('http://', 'https://', '//')):
        url = urljoin(base_url, url)
    if url.startswith('//'): url = 'https:' + url
    parsed = urlparse(url)
    if not parsed.scheme or not parsed.netloc: return ""
    return url

def parse_dt(v: Any) -> Optional[datetime]:
    """ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë‚ ì§œ/ì‹œê°„ íŒŒì‹± (ê°œì„ ëœ ë²„ì „ ìœ ì§€)"""
    if not v: return None
    if isinstance(v, datetime):
        return v if v.tzinfo else v.replace(tzinfo=timezone.utc)
    if isinstance(v, (int, float)):
        try:
            ts = v / 1000 if v > 10_000_000_000 else v
            return datetime.fromtimestamp(ts, tz=timezone.utc)
        except (ValueError, OSError): return None
    if isinstance(v, str):
        v = v.strip()
        if not v: return None
        v = v.replace("Z", "+00:00")
        try:
            dt_obj = datetime.fromisoformat(v)
            return dt_obj if dt_obj.tzinfo else dt_obj.replace(tzinfo=timezone.utc)
        except ValueError: pass
        date_formats = [
            "%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M", "%Y-%m-%d",
            "%Y/%m/%d %H:%M:%S", "%Y/%m/%d %H:%M", "%Y/%m/%d",
            "%Y.%m.%d %H:%M:%S", "%Y.%m.%d %H:%M", "%Y.%m.%d",
            "%d/%m/%Y %H:%M:%S", "%d/%m/%Y", "%d-%m-%Y",
            "%Yë…„ %mì›” %dì¼ %H:%M", "%Yë…„ %mì›” %dì¼",
        ]
        for fmt in date_formats:
            try:
                dt_obj = datetime.strptime(v, fmt)
                return dt_obj.replace(tzinfo=timezone.utc)
            except ValueError: continue
    return None

def extract_field(item: Dict[str, Any], field_names: List[str], default: Any = "") -> Optional[Any]:
    """ì—¬ëŸ¬ ê°€ëŠ¥í•œ í•„ë“œëª…ì—ì„œ ê°’ ì¶”ì¶œ"""
    for field in field_names:
        if '.' in field:
            parts = field.split('.')
            value = item
            for part in parts:
                if isinstance(value, dict): value = value.get(part)
                else: value = None; break
            if value is not None: return value # ê°’ ìì²´ê°€ Noneì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ Noneê³¼ ë¹„êµ
        else:
            value = item.get(field)
            if value is not None: return value
    return default

def normalize_item(item: dict, base_url: Optional[str] = None) -> dict:
    """ì•„ì´í…œ ì •ê·œí™” (ê°œì„ ëœ ë²„ì „ ìœ ì§€)"""
    title = clean_text(extract_field(item, ["title", "name", "subject", "headline", "meta.title", "og:title", "titleText"]), max_length=500)
    url = normalize_url(extract_field(item, ["url", "link", "href", "permalink", "canonical", "meta.url", "og:url"]), base_url)
    summary_raw = clean_text(extract_field(item, ["summary", "description", "excerpt", "preview", "meta.description", "og:description", "abstract"]), max_length=1000)
    body_html = extract_field(item, ["html", "content_html", "body_html", "htmlContent"])
    body_text = extract_field(item, ["text", "content", "body", "body_text", "plainText"])
    if body_html and not body_text: body_text = extract_text_from_html(body_html)
    if body_text and not summary_raw: summary_raw = clean_text(body_text[:500])
    published_at = None
    date_fields = ["publishedAt", "published_at", "pubDate", "date", "datetime", "time", "createdAt", "created_at", "timestamp", "postDate", "releaseDate"]
    for field in date_fields:
        value = item.get(field)
        if value:
            published_at = parse_dt(value)
            if published_at: break
    result = {
        "title": title, "url": url, "summary_raw": summary_raw,
        "body_html": body_html, "body_text": body_text, "published_at": published_at,
    }
    category = extract_field(item, ["category", "categories", "tag", "tags", "section"])
    author = extract_field(item, ["author", "writer", "creator", "by"])
    if category: result["category"] = clean_text(str(category))
    if author: result["author"] = clean_text(str(author))
    return result

def validate_normalized_item(item: dict) -> bool:
    """ì •ê·œí™”ëœ ì•„ì´í…œì˜ ìœ íš¨ì„± ê²€ì¦"""
    if not item.get("title") or not item.get("url"): return False
    if not item["url"].startswith(('http://', 'https://')): return False
    if len(item["title"]) < 3: return False
    if item.get("published_at") and item["published_at"] > datetime.now(timezone.utc): return False
    return True

def content_hash(college_key: str, title: str, url: str, published_at: Optional[datetime]) -> str:
    """ì»¨í…ì¸  í•´ì‹œ ìƒì„± (ê°œì„ ëœ ë²„ì „ ìœ ì§€)"""
    url = url.rstrip('/')
    date_str = published_at.date().isoformat() if published_at else ""
    title_normalized = re.sub(r'\s+', ' ', title.lower().strip())
    base = f"{college_key}|{title_normalized}|{url}|{date_str}"
    return hashlib.sha256(base.encode('utf-8')).hexdigest()
# --- ê¸°ì¡´ í—¬í¼ í•¨ìˆ˜ ì¢…ë£Œ ---

# ==============================================================================
# Apify API í—¬í¼ í•¨ìˆ˜ (ìµœì‹  ì‹¤í–‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°)
# ==============================================================================
def get_latest_run_for_task(task_id: str, timeout=30):
    """GET /v2/actor-tasks/{taskId}/runs - ê°€ì¥ ìµœê·¼ ì„±ê³µí•œ ì‹¤í–‰ 1ê°œë§Œ ê°€ì ¸ì˜¤ê¸°"""
    url = f"https://api.apify.com/v2/actor-tasks/{task_id}/runs"
    params = {"token": APIFY_TOKEN, "limit": 1, "desc": "true"}
    try:
        resp = SESSION.get(url, params=params, timeout=timeout)
        resp.raise_for_status() # HTTP ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
        data = resp.json()
        runs = data.get("data", {}).get("items", [])
    except requests.RequestException as e:
        print(f"  âŒ get runs error for task {task_id}: {e}")
        return None
    except json.JSONDecodeError:
        print(f"  âš ï¸ get runs: invalid JSON response for task {task_id}")
        return None

    if not runs:
        print(f"  âš ï¸ No recent run found for task {task_id}")
        return None

    latest_run = runs[0]
    if latest_run.get("status") == "SUCCEEDED":
        return latest_run
    else:
        status = latest_run.get("status", "UNKNOWN")
        print(f"  âš ï¸ Latest run for task {task_id} was not successful (status: {status})")
        return None

def fetch_dataset_items(dataset_id: str, timeout=300):
    """ë°ì´í„°ì…‹ ì•„ì´í…œ ê°€ì ¸ì˜¤ê¸°"""
    url = f"https://api.apify.com/v2/datasets/{dataset_id}/items"
    params = {"token": APIFY_TOKEN, "format": "json", "clean": "true"}
    try:
        resp = SESSION.get(url, params=params, timeout=timeout)
        resp.raise_for_status()
        data = resp.json()
        # Apify ì‘ë‹µ í˜•ì‹ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ ê²½ìš° ì²˜ë¦¬
        if isinstance(data, dict) and 'items' in data:
            return data['items']
        elif isinstance(data, list):
             return data
        else:
             print(f"  âš ï¸ Unexpected data format from dataset {dataset_id}: {type(data)}")
             return []
    except requests.RequestException as e:
        print(f"  âš ï¸ items fetch error for dataset {dataset_id}: {e}")
    except json.JSONDecodeError:
        print(f"  âš ï¸ items JSON decode error for dataset {dataset_id}")
    return []

# ==============================================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ìµœì‹  ë°ì´í„° ê°€ì ¸ì˜¤ê¸° + ìµœì‹  AI ì²˜ë¦¬)
# ==============================================================================
def run():
    total_upserted = 0
    total_skipped = 0

    print(f"ğŸ¤– AI_IN_PIPELINE: {AI_IN_PIPELINE}")
    print(f"â±ï¸ AI_SLEEP_SEC: {AI_SLEEP_SEC}")
    print(f"ğŸ”¢ AI_MAX_PER_COLLEGE: {AI_MAX_PER_COLLEGE}")

    conn = None
    try:
        conn = psycopg2.connect(DATABASE_URL)
        conn.autocommit = False # ëª…ì‹œì  ì»¤ë°‹ ì‚¬ìš©

        with conn.cursor(cursor_factory=RealDictCursor) as cur:
            for ck, meta in COLLEGES.items():
                name = meta["name"]
                task_id = meta["task_id"]
                site = meta.get("url")

                print(f"\nğŸ” Fetching latest run for: {name} ({ck})")

                run_data = get_latest_run_for_task(task_id)
                if not run_data:
                    continue

                run_id = run_data.get("id")
                ds_id = run_data.get("defaultDatasetId")
                finished_at = run_data.get("finishedAt", "unknown")

                if not ds_id:
                    print(f"  âŒ no datasetId for run {run_id} ({ck})")
                    continue

                print(f"  ğŸ“… Using run {run_id} finished at {finished_at}")

                items = fetch_dataset_items(ds_id)
                if not items:
                     print(f"  âš ï¸ No items found in dataset {ds_id}")
                     continue
                print(f"  ğŸ“¦ Items fetched: {len(items)}")

                college_upserted = 0
                college_skipped = 0
                ai_call_count = 0

                for rec in items:
                    norm = normalize_item(rec, base_url=site)

                    if not validate_normalized_item(norm):
                        college_skipped += 1
                        continue

                    # ============================================
                    # AI ì²˜ë¦¬ ë¡œì§ ìˆ˜ì • (ìµœì‹  ai_processor ì‚¬ìš©)
                    # ============================================
                    if AI_IN_PIPELINE and ai_call_count < AI_MAX_PER_COLLEGE:
                        time.sleep(AI_SLEEP_SEC) # API í˜¸ì¶œ ì „ ì§€ì—°
                        try:
                            title_for_ai = norm.get("title", "").strip()
                            body_for_ai = norm.get("body_text", "").strip()

                            # 1ë‹¨ê³„: ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
                            category_ai = classify_notice_category(title=title_for_ai, body=body_for_ai)
                            norm["category_ai"] = category_ai

                            # 2ë‹¨ê³„: êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ (ë¶„ë¥˜ëœ ì¹´í…Œê³ ë¦¬ ì‚¬ìš©)
                            structured_info = extract_structured_info(title=title_for_ai, body=body_for_ai, category=category_ai)

                            # --- structured_infoì—ì„œ í•„ë“œ ì¶”ì¶œ (main.py ë¡œì§ ì°¸ê³ ) ---
                            # í˜„ì¬ start_at/end_atì€ structured_infoì—ì„œ ì§ì ‘ íŒŒì‹±í•˜ì§€ ì•ŠìŒ
                            start_at_ai = None # í•„ìš” ì‹œ calendar_utils ë“± í™œìš© ë¡œì§ ì¶”ê°€
                            end_at_ai = None   # í•„ìš” ì‹œ calendar_utils ë“± í™œìš© ë¡œì§ ì¶”ê°€

                            # qualification_aiëŠ” JSON ê°ì²´ ë˜ëŠ” ë¹ˆ ë”•ì…”ë„ˆë¦¬
                            # structured_info ìì²´ê°€ qualificationì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í™•ì¸
                            qualification_ai = structured_info.get("qualifications",
                                                                 structured_info if isinstance(structured_info, dict) and "error" not in structured_info else {})

                            # hashtags_aiëŠ” category_ai ê¸°ë°˜ ë¦¬ìŠ¤íŠ¸ (main.pyì™€ ë™ì¼í•˜ê²Œ, #ì¼ë°˜ ì œì™¸)
                            hashtags_ai = [category_ai] if category_ai and category_ai != "#ì¼ë°˜" else None

                            norm["start_at_ai"] = start_at_ai
                            norm["end_at_ai"] = end_at_ai
                            norm["qualification_ai"] = qualification_ai
                            norm["hashtags_ai"] = hashtags_ai

                            ai_call_count += 1

                        except Exception as e:
                            # 429 ê°ì§€ ì‹œ ì¶”ê°€ ìŠ¬ë¦½
                            if isinstance(e, requests.exceptions.HTTPError) and e.response.status_code == 429:
                                print(f"  âš ï¸ 429 Rate limit detected, sleeping 5 seconds...")
                                time.sleep(5.0)
                            elif "429" in str(e): # Gemini APIì˜ ê²½ìš° ë‹¤ë¥¸ í˜•íƒœì¼ ìˆ˜ ìˆìŒ
                                print(f"  âš ï¸ 429 Rate limit detected (non-request), sleeping 5 seconds...")
                                time.sleep(5.0)

                            print(f"  âš ï¸ AI extraction failed for '{norm.get('title', 'N/A')[:50]}...': {e}. Proceeding without AI data.")
                            # ì‹¤íŒ¨ ì‹œ AI í•„ë“œëŠ” None/ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
                            norm["category_ai"] = None
                            norm["start_at_ai"] = None
                            norm["end_at_ai"] = None
                            norm["qualification_ai"] = {}
                            norm["hashtags_ai"] = None
                    else:
                        # AI ë¹„í™œì„±í™” ë˜ëŠ” ë°°ì¹˜ ì œí•œ ì´ˆê³¼ ì‹œ
                        norm["category_ai"] = None
                        norm["start_at_ai"] = None
                        norm["end_at_ai"] = None
                        norm["qualification_ai"] = {}
                        norm["hashtags_ai"] = None

                    h = content_hash(ck, norm["title"], norm["url"], norm.get("published_at"))

                    # DB ì €ì¥ (AI í•„ë“œ ë° search_vector í¬í•¨)
                    try:
                        cur.execute(UPSERT_SQL, {
                            "college_key": ck,
                            "title": norm["title"],
                            "url": norm["url"],
                            "summary_raw": norm.get("summary_raw"),
                            "body_html": norm.get("body_html"),
                            "body_text": norm.get("body_text"),
                            "published_at": norm.get("published_at"),
                            "source_site": site,
                            "content_hash": h,
                            "category_ai": norm.get("category_ai"),
                            "start_at_ai": norm.get("start_at_ai"),
                            "end_at_ai": norm.get("end_at_ai"),
                            # qualification_aiëŠ” Json()ìœ¼ë¡œ ê°ì‹¸ì„œ ì „ë‹¬
                            "qualification_ai": Json(norm.get("qualification_ai") or {}),
                            "hashtags_ai": norm.get("hashtags_ai"),
                        })
                        if cur.rowcount > 0:
                            college_upserted += 1
                        # else: # ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ì—…ë°ì´íŠ¸ë˜ì§€ ì•Šì€ ê²½ìš° (rowcountê°€ 0ì¼ ìˆ˜ ìˆìŒ)
                        #     pass

                    except psycopg2.Error as db_err:
                        conn.rollback() # í˜„ì¬ ì•„ì´í…œ ë¡¤ë°±
                        print(f"  âŒ DB error upserting '{norm.get('title', 'N/A')[:50]}...': {db_err}")
                        college_skipped += 1
                        # ë‹¤ìŒ ì•„ì´í…œ ì²˜ë¦¬ë¥¼ ìœ„í•´ autocommit ìƒíƒœ ë³µì› (í•„ìš” ì‹œ)
                        # conn.autocommit = False # ë£¨í”„ ì‹œì‘ ì‹œ ì„¤ì •í–ˆìœ¼ë¯€ë¡œ ë¶ˆí•„ìš”í•  ìˆ˜ ìˆìŒ
                    except Exception as general_err:
                         conn.rollback() # ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ì‹œ ë¡¤ë°±
                         print(f"  âŒ Unexpected error during upsert for '{norm.get('title', 'N/A')[:50]}...': {general_err}")
                         college_skipped += 1

                # í•œ ëŒ€í•™ ì²˜ë¦¬ í›„ ì»¤ë°‹
                conn.commit()
                print(f"  âœ… {name}: upserted={college_upserted}, skipped={college_skipped}, ai_calls={ai_call_count}")

                total_upserted += college_upserted
                total_skipped += college_skipped

    except psycopg2.Error as db_conn_err:
        print(f"\nâŒ Database connection error: {db_conn_err}")
    except KeyboardInterrupt:
        print("\nğŸš« Operation cancelled by user.")
        if conn: conn.rollback() # ì¤‘ë‹¨ ì‹œ ë¡¤ë°± ì‹œë„
    except Exception as e:
        print(f"\nâŒ An unexpected error occurred: {e}")
        if conn: conn.rollback() # ì˜ˆì™¸ ë°œìƒ ì‹œ ë¡¤ë°± ì‹œë„
    finally:
        if conn:
            conn.close() # ì»¤ë„¥ì…˜ ë°˜í™˜

    print(f"\nâœ¨ Total: upserted={total_upserted}, skipped={total_skipped}")

if __name__ == "__main__":
    run()