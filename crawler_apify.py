# crawler_apify.py (ìˆ˜ì •ëœ run í•¨ìˆ˜ í¬í•¨)
import os
import time
import json
import hashlib
import requests
import psycopg2
import re
import datetime as dt
from datetime import datetime, timezone
from urllib.parse import urlencode, urlparse, urljoin
from psycopg2.extras import RealDictCursor, Json
from dotenv import load_dotenv
from typing import Optional, Dict, Any, List, Tuple
from html import unescape
from bs4 import BeautifulSoup
from colleges import COLLEGES
import logging # ë¡œê¹… ì„í¬íŠ¸ ì¶”ê°€

# AI processor import ìˆ˜ì • (ë°°ì¹˜ ë¶„ë¥˜ í•¨ìˆ˜ ì‚¬ìš©)
from ai_processor import (
    classify_hashtags_from_title_batch,
    clean_json_string # clean_json_string ì„í¬íŠ¸ ì¶”ê°€ (ai_processor.pyì— ìˆ˜ì •ëœ í•¨ìˆ˜ê°€ ìˆë‹¤ê³  ê°€ì •)
)
# _to_utc_ts í•¨ìˆ˜ import
try:
    from main import _to_utc_ts
except ImportError:
    print("Warning: Could not import _to_utc_ts from main.py. Defining locally.")
    def _to_utc_ts(date_yyyy_mm_dd: str | None):
        if not date_yyyy_mm_dd:
            return None
        try:
            d = dt.date.fromisoformat(date_yyyy_mm_dd)
            return datetime(d.year, d.month, d.day, tzinfo=timezone.utc)
        except (ValueError, TypeError):
            print(f"Warning: Invalid date format: {date_yyyy_mm_dd}. Returning None.")
            return None


load_dotenv(encoding="utf-8")

APIFY_TOKEN = os.getenv("APIFY_TOKEN")
DATABASE_URL = os.getenv("DATABASE_URL")
AI_IN_PIPELINE = os.getenv("AI_IN_PIPELINE", "true").lower() == "true"
AI_SLEEP_SEC = float(os.getenv("AI_SLEEP_SEC", "1.0"))
AI_BATCH_SIZE = int(os.getenv("AI_BATCH_SIZE", "10"))

if not APIFY_TOKEN:
    raise RuntimeError("APIFY_TOKEN not set")
if not DATABASE_URL:
    raise RuntimeError("DATABASE_URL not set")

# ë¡œê±° ì„¤ì • ì¶”ê°€
logger = logging.getLogger(__name__) # __name__ìœ¼ë¡œ ë¡œê±° ì´ë¦„ ì„¤ì •
logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s")


SESSION = requests.Session()
SESSION.headers.update({"Accept": "application/json"})

# AI í•„ë“œ í¬í•¨ëœ UPSERT SQL (ê¸°ì¡´ê³¼ ë™ì¼)
UPSERT_SQL = """
INSERT INTO notices (
    college_key, title, url, summary_raw, body_html, body_text,
    published_at, source_site, content_hash,
    category_ai, start_at_ai, end_at_ai, qualification_ai, hashtags_ai,
    search_vector
) VALUES (
    %(college_key)s, %(title)s, %(url)s, %(summary_raw)s,
    %(body_html)s, %(body_text)s, %(published_at)s,
    %(source_site)s, %(content_hash)s,
    %(category_ai)s, %(start_at_ai)s, %(end_at_ai)s, %(qualification_ai)s, %(hashtags_ai)s,
    setweight(to_tsvector('simple', coalesce(%(title)s, '')), 'A') ||
    setweight(to_tsvector('simple', coalesce(array_to_string(%(hashtags_ai)s, ' '), '')), 'B') ||
    setweight(to_tsvector('simple', coalesce(%(body_text)s, '')), 'C')
)
ON CONFLICT (content_hash)
DO UPDATE SET
    title = EXCLUDED.title,
    url = EXCLUDED.url,
    summary_raw = EXCLUDED.summary_raw,
    body_html = EXCLUDED.body_html,
    body_text = EXCLUDED.body_text,
    published_at = EXCLUDED.published_at,
    category_ai = EXCLUDED.category_ai, -- category_ai ì¶”ê°€
    start_at_ai = EXCLUDED.start_at_ai, -- start_at_ai ì¶”ê°€
    end_at_ai = EXCLUDED.end_at_ai,   -- end_at_ai ì¶”ê°€
    qualification_ai = EXCLUDED.qualification_ai, -- qualification_ai ì¶”ê°€
    hashtags_ai = EXCLUDED.hashtags_ai, -- hashtags_ai ì¶”ê°€
    updated_at = CURRENT_TIMESTAMP,
    search_vector = setweight(to_tsvector('simple', coalesce(EXCLUDED.title, '')), 'A') ||
                    setweight(to_tsvector('simple', coalesce(array_to_string(EXCLUDED.hashtags_ai, ' '), '')), 'B') ||
                    setweight(to_tsvector('simple', coalesce(EXCLUDED.body_text, '')), 'C')
RETURNING id; -- Optional: ë°˜í™˜ ê°’ ì¶”ê°€í•˜ì—¬ ì—…ë°ì´íŠ¸ í™•ì¸ ê°€ëŠ¥
"""

# --- ê¸°ì¡´ í—¬í¼ í•¨ìˆ˜ë“¤ (clean_text, extract_text_from_html, ...) ---
# ì—¬ê¸°ì— ê¸°ì¡´ í—¬í¼ í•¨ìˆ˜ë“¤ì´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤. (ì½”ë“œê°€ ë„ˆë¬´ ê¸¸ì–´ì ¸ ìƒëµ)
def clean_text(text: Optional[str], max_length: Optional[int] = None) -> str:
    if not text: return ""
    text = unescape(text) # HTML ì—”í‹°í‹° ë””ì½”ë”©
    text = re.sub(r'\s+', ' ', text) # ì—°ì† ê³µë°±ì„ í•˜ë‚˜ë¡œ
    text = text.strip() # ì•ë’¤ ê³µë°± ì œê±°
    if max_length and len(text) > max_length: # ê¸¸ì´ ì œí•œ
        text = text[:max_length-3] + "..."
    return text

def extract_text_from_html(html: Optional[str]) -> str:
    """HTMLì—ì„œ ì£¼ìš” í…ìŠ¤íŠ¸ ë‚´ìš©ë§Œ ì¶”ì¶œ ì‹œë„"""
    if not html: return ""
    try:
        # íŠ¹ì • íŒ¨í„´('ê²Œì‹œê¸€ ë‚´ìš©' ~ 'ëª©ë¡') ì‚¬ì´ ë‚´ìš© ìš°ì„  ì¶”ì¶œ ì‹œë„
        content_pattern = r'ê²Œì‹œê¸€ ë‚´ìš©(.*?)ëª©ë¡'
        content_match = re.search(content_pattern, html, re.DOTALL)
        soup_text = html
        if content_match:
            soup_text = content_match.group(1).strip()

        # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
        soup = BeautifulSoup(soup_text, 'html.parser')

        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for tag in soup(['script', 'style', 'meta', 'link', 'header', 'footer', 'nav', 'aside']):
            tag.decompose()

        # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ë¦¬
        text = soup.get_text(separator=' ', strip=True)
        # ë¶ˆí•„ìš”í•œ íŒŒì´í”„ ë¬¸ì ì •ë¦¬ (ì˜ˆ: ' | ')
        text = re.sub(r'(\s*\|\s*)+', ' ', text)
        return clean_text(text)
    except Exception as e:
        logger.warning(f"  âš ï¸ HTML parsing error: {e}")
        return "" # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜

def normalize_url(url: Optional[str], base_url: Optional[str] = None) -> str:
    """URL ì •ê·œí™” (ì ˆëŒ€ ê²½ë¡œ ë³€í™˜, fragment ì œê±° ë“±)"""
    if not url: return ""
    url = url.strip()

    # ìƒëŒ€ ê²½ë¡œ -> ì ˆëŒ€ ê²½ë¡œ ë³€í™˜
    if base_url and not url.startswith(('http://', 'https://', '//', 'javascript:')):
        try:
            url = urljoin(base_url, url)
        except ValueError:
             logger.warning(f"  âš ï¸ Could not join base_url '{base_url}' and relative url '{url}'")
             return "" # URL ê²°í•© ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´

    # í”„ë¡œí† ì½œ ì—†ëŠ” URL ì²˜ë¦¬ (ì˜ˆ: //example.com)
    if url.startswith('//'):
        url = 'https:' + url

    # javascript: ë§í¬ ë¬´ì‹œ
    if url.startswith('javascript:'):
        return ""

    # URL íŒŒì‹±í•˜ì—¬ ìœ íš¨ì„± ê²€ì‚¬ (scheme, netloc í™•ì¸)
    parsed = urlparse(url)
    if not parsed.scheme or not parsed.netloc:
        logger.debug(f"  âš ï¸ Invalid URL structure: {url}")
        return ""

    # Fragment ì œê±° (ì˜ˆ: #section1)
    url = url.split('#')[0]

    return url

def parse_dt(v: Any) -> Optional[datetime]:
    """ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë‚ ì§œ/ì‹œê°„ ê°’ì„ UTC datetime ê°ì²´ë¡œ íŒŒì‹±"""
    if not v: return None

    # ì´ë¯¸ datetime ê°ì²´ì¸ ê²½ìš°
    if isinstance(v, datetime):
        # íƒ€ì„ì¡´ ì •ë³´ê°€ ì—†ìœ¼ë©´ UTCë¡œ ê°„ì£¼
        return v if v.tzinfo else v.replace(tzinfo=timezone.utc)

    # íƒ€ì„ìŠ¤íƒ¬í”„ (ì´ˆ ë˜ëŠ” ë°€ë¦¬ì´ˆ)ì¸ ê²½ìš°
    if isinstance(v, (int, float)):
        try:
            # ë°€ë¦¬ì´ˆì¸ì§€ ì´ˆì¸ì§€ ì¶”ì •
            ts = v / 1000 if v > 10_000_000_000 else v
            return datetime.fromtimestamp(ts, tz=timezone.utc)
        except (ValueError, OSError): # ìœ íš¨í•˜ì§€ ì•Šì€ íƒ€ì„ìŠ¤íƒ¬í”„
            logger.warning(f"  âš ï¸ Invalid timestamp: {v}")
            return None

    # ë¬¸ìì—´ì¸ ê²½ìš°
    if isinstance(v, str):
        v = v.strip()
        if not v: return None

        # ISO 8601 í˜•ì‹ (+/-HH:MM ë˜ëŠ” Z í¬í•¨) ìš°ì„  ì²˜ë¦¬
        if re.search(r'[+-]\d{2}:\d{2}$|Z$', v):
            v = v.replace("Z", "+00:00") # Zë¥¼ UTC ì˜¤í”„ì…‹ìœ¼ë¡œ ë³€ê²½
            try:
                # íŒŒì‹± ì „ ë§ˆì´í¬ë¡œì´ˆ ë¶€ë¶„ ê¸¸ì´ ì¡°ì • (ìµœëŒ€ 6ìë¦¬)
                if '.' in v and ('+' in v or ('-' in v and v.rfind('-') > v.find('T'))):
                    parts = v.rsplit('+', 1) if '+' in v else v.rsplit('-', 1)
                    time_part = parts[0]
                    tz_part = parts[1]
                    if '.' in time_part:
                       time_part = time_part[:time_part.find('.')+7] # ë§ˆì´í¬ë¡œì´ˆ 6ìë¦¬ê¹Œì§€ë§Œ
                    v = f"{time_part}{'+' if '+' in v else '-'}{tz_part}"

                dt_obj = datetime.fromisoformat(v)
                return dt_obj.astimezone(timezone.utc) # UTCë¡œ ë³€í™˜
            except ValueError as e:
                logger.debug(f"  âš ï¸ ISO format parse error for '{v}': {e}")
                pass # ë‹¤ë¥¸ í˜•ì‹ ì‹œë„

        # ë‹¤ì–‘í•œ ì¼ë°˜ ë‚ ì§œ/ì‹œê°„ í˜•ì‹ ì‹œë„
        date_formats = [
            "%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M", "%Y-%m-%d",
            "%Y/%m/%d %H:%M:%S", "%Y/%m/%d %H:%M", "%Y/%m/%d",
            "%Y.%m.%d %H:%M:%S", "%Y.%m.%d %H:%M", "%Y.%m.%d", # ì  êµ¬ë¶„
            "%d/%m/%Y %H:%M:%S", "%d/%m/%Y", "%d-%m-%Y", # DD/MM/YYYY í˜•ì‹
            "%Yë…„ %mì›” %dì¼ %H:%M", "%Yë…„ %mì›” %dì¼", # í•œêµ­ì–´ í˜•ì‹
        ]
        for fmt in date_formats:
            try:
                dt_obj = datetime.strptime(v, fmt)
                # íƒ€ì„ì¡´ ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ UTCë¡œ ì„¤ì •
                return dt_obj.replace(tzinfo=timezone.utc)
            except ValueError:
                continue # ë‹¤ìŒ í˜•ì‹ ì‹œë„

    # ì–´ë–¤ í˜•ì‹ì—ë„ ë§ì§€ ì•Šìœ¼ë©´ None ë°˜í™˜
    logger.debug(f"  âš ï¸ Unparseable date format: {v} (type: {type(v)})")
    return None

def extract_field(item: Dict[str, Any], field_names: List[str], default: Any = "") -> Optional[Any]:
    """ì—¬ëŸ¬ í•„ë“œ ì´ë¦„ í›„ë³´ ì¤‘ ì²« ë²ˆì§¸ë¡œ ì°¾ì€ ê°’ì„ ë°˜í™˜ (ì  í‘œê¸°ë²• ì§€ì›)"""
    for field in field_names:
        if '.' in field: # ì¤‘ì²©ëœ í•„ë“œ ì ‘ê·¼ (ì˜ˆ: 'meta.title')
            parts = field.split('.')
            value = item
            valid_path = True
            for part in parts:
                if isinstance(value, dict):
                    value = value.get(part)
                elif isinstance(value, list) and part.isdigit() and int(part) < len(value):
                    # ë¦¬ìŠ¤íŠ¸ ì¸ë±ìŠ¤ ì ‘ê·¼ (ì œí•œì  ì§€ì›)
                    value = value[int(part)]
                else:
                    value = None
                    valid_path = False
                    break
            if valid_path and value is not None:
                return value
        else: # ì¼ë°˜ í•„ë“œ ì ‘ê·¼
            value = item.get(field)
            if value is not None:
                return value
    # ëª¨ë“  í›„ë³´ í•„ë“œì— ê°’ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ë°˜í™˜
    return default

def normalize_item(item: dict, base_url: Optional[str] = None) -> dict:
    """Apify í¬ë¡¤ë§ ê²°ê³¼ itemì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”"""
    # ì œëª© ì¶”ì¶œ (ì—¬ëŸ¬ í•„ë“œëª… í›„ë³´ ì‚¬ìš©)
    title = clean_text(extract_field(item, ["title", "name", "subject", "headline", "meta.title", "og:title", "titleText", "h1", "h2"], default=""), max_length=500)
    # URL ì¶”ì¶œ ë° ì •ê·œí™”
    url = normalize_url(extract_field(item, ["url", "link", "href", "permalink", "canonical", "meta.url", "og:url"], default=""), base_url)
    # ìš”ì•½ ì¶”ì¶œ
    summary_raw = clean_text(extract_field(item, ["summary", "description", "excerpt", "preview", "meta.description", "og:description", "abstract"], default=""), max_length=1000)
    # HTML ë³¸ë¬¸ ì¶”ì¶œ
    body_html = extract_field(item, ["html", "content_html", "body_html", "htmlContent", "content", "text"], default=None) # HTMLì€ ê·¸ëŒ€ë¡œ ìœ ì§€ ì‹œë„
    # í…ìŠ¤íŠ¸ ë³¸ë¬¸ ì¶”ì¶œ (raw text ìš°ì„ , ì—†ìœ¼ë©´ HTMLì—ì„œ ì¶”ì¶œ)
    body_text_raw = extract_field(item, ["text", "content", "body", "body_text", "plainText"], default=None)
    body_text = clean_text(body_text_raw) # raw text í´ë¦¬ë‹

    # HTMLì´ ìˆê³ , ê±°ê¸°ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ê°€ ë” ê¸¸ê±°ë‚˜ raw textê°€ ì—†ìœ¼ë©´ HTML ê¸°ë°˜ í…ìŠ¤íŠ¸ ì‚¬ìš©
    if body_html:
        body_text_from_html = extract_text_from_html(body_html)
        if len(body_text_from_html) > len(body_text):
            body_text = body_text_from_html
        elif not body_text and body_text_from_html: # raw textê°€ ì—†ê³  HTML textë§Œ ìˆì„ ë•Œ
            body_text = body_text_from_html

    # ìš”ì•½ì´ ì—†ê³  ë³¸ë¬¸ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ë³¸ë¬¸ ì•ë¶€ë¶„ì„ ìš”ì•½ìœ¼ë¡œ ì‚¬ìš©
    if body_text and not summary_raw:
        summary_raw = clean_text(body_text[:500]) # 500ìë¡œ ì œí•œ

    # ë°œí–‰ì¼ ì¶”ì¶œ (ì—¬ëŸ¬ í•„ë“œëª… í›„ë³´ ì‚¬ìš© ë° íŒŒì‹±)
    published_at = None
    date_fields = ["publishedAt", "published_at", "createdAt", "created_at", "datetime", "timestamp", "pubDate", "date", "time", "postDate", "releaseDate"]
    for field in date_fields:
        value = extract_field(item, [field], default=None) # ì¤‘ì²© í•„ë“œë„ ê°€ëŠ¥í•˜ê²Œ extract_field ì‚¬ìš©
        if value:
            parsed = parse_dt(value)
            if parsed:
                # ë„ˆë¬´ ì˜¤ë˜ëœ ë‚ ì§œëŠ” ì˜¤ë¥˜ ê°€ëŠ¥ì„± ìˆìœ¼ë¯€ë¡œ ë¡œê·¸ ë‚¨ê¸°ê³  ë¬´ì‹œ (ì˜ˆ: 1990ë…„ ì´ì „)
                if parsed.year >= 1990:
                    published_at = parsed
                    break # ì²« ë²ˆì§¸ ì„±ê³µí•œ íŒŒì‹± ê²°ê³¼ ì‚¬ìš©
                else:
                     logger.debug(f"  âš ï¸ Skipping date parse due to unlikely year: {parsed} from field '{field}'")

    # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
    result = {
        "title": title,
        "url": url,
        "summary_raw": summary_raw,
        "body_html": body_html,
        "body_text": body_text,
        "published_at": published_at, # íŒŒì‹±ëœ datetime ê°ì²´ ë˜ëŠ” None
    }
    return result

def validate_normalized_item(item: dict) -> bool:
    """ì •ê·œí™”ëœ itemì˜ í•„ìˆ˜ í•„ë“œ ë° ìœ íš¨ì„± ê²€ì¦"""
    # ì œëª©ê³¼ URLì€ í•„ìˆ˜
    if not item.get("title") or not item.get("url"):
        logger.warning(f"  âš ï¸ Skipping item due to missing title or URL: {item.get('url') or 'No URL'}")
        return False

    # URL í˜•ì‹ ê²€ì¦ (scheme, netloc)
    parsed_url = urlparse(item["url"])
    if not parsed_url.scheme or not parsed_url.netloc:
        logger.warning(f"  âš ï¸ Skipping item due to invalid URL: {item['url']}")
        return False

    # ë„ˆë¬´ ì§§ì€ ì œëª© ë¬´ì‹œ (ì˜¤ë¥˜ ê°€ëŠ¥ì„±)
    if len(item["title"]) < 3:
         logger.debug(f"  âš ï¸ Skipping item due to short title: {item['title']}")
         return False

    # ë°œí–‰ì¼ ìœ íš¨ì„± ê²€ì¦ (datetime ê°ì²´ì´ê³ , ë„ˆë¬´ ì˜¤ë˜ë˜ì§€ ì•ŠìŒ)
    if item.get("published_at"):
        pub_dt = item["published_at"]
        # datetime ê°ì²´ê°€ ì•„ë‹ˆë©´ ì‹¤íŒ¨
        if not isinstance(pub_dt, datetime):
             logger.warning(f"  âš ï¸ Skipping item due to invalid date type: {type(pub_dt)}")
             return False
        # íƒ€ì„ì¡´ ì •ë³´ ê°•ì œ (UTC)
        if pub_dt.tzinfo is None or pub_dt.tzinfo.utcoffset(pub_dt) is None:
             item["published_at"] = pub_dt.replace(tzinfo=timezone.utc) # ì›ë³¸ item ë”•ì…”ë„ˆë¦¬ ì§ì ‘ ìˆ˜ì •
        # ë„ˆë¬´ ì˜¤ë˜ëœ ë‚ ì§œ ë¬´ì‹œ
        if item["published_at"].year < 1990:
             logger.debug(f"  âš ï¸ Skipping item due to very old date: {item['published_at']}")
             return False

    # ëª¨ë“  ê²€ì¦ í†µê³¼
    return True

def content_hash(college_key: str, title: str, url: str, published_at: Optional[datetime]) -> str:
    """ê³µì§€ì‚¬í•­ ë‚´ìš© ê¸°ë°˜ ê³ ìœ  í•´ì‹œ ìƒì„± (ì¤‘ë³µ ë°©ì§€ìš©)"""
    url = url.rstrip('/') # URL ë '/' ì œê±°í•˜ì—¬ ì •ê·œí™”
    date_str = published_at.strftime('%Y-%m-%d') if published_at else "" # ë‚ ì§œ ë¶€ë¶„ë§Œ ì‚¬ìš© (ì‹œê°„ ì œì™¸)
    # ì œëª© ì •ê·œí™”: íŠ¹ì • ë¬¸ì ì œê±°, ì†Œë¬¸ì ë³€í™˜, ê³µë°± ì •ë¦¬
    title_normalized = re.sub(r'[\'\"\[\]â€˜â€™â€œâ€]', '', title) # ë”°ì˜´í‘œ, ëŒ€ê´„í˜¸ ì œê±°
    title_normalized = re.sub(r'\s+', ' ', title_normalized.lower().strip()) # ì†Œë¬¸ì, ê³µë°± ì •ë¦¬

    # í•´ì‹œ ìƒì„± ê¸°ì¤€ ë¬¸ìì—´ ì¡°í•©
    base = f"{college_key}|{title_normalized}|{url}|{date_str}"
    h = hashlib.sha256(base.encode('utf-8')).hexdigest()
    return h


# --- ê¸°ì¡´ Apify í—¬í¼ í•¨ìˆ˜ (get_latest_run_for_task, fetch_dataset_items) ---
# ì—¬ê¸°ì— ê¸°ì¡´ Apify ê´€ë ¨ í•¨ìˆ˜ë“¤ì´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤. (ì½”ë“œê°€ ë„ˆë¬´ ê¸¸ì–´ì ¸ ìƒëµ)
def get_latest_run_for_task(task_id: str, timeout=60):
    """Apify Taskì˜ ê°€ì¥ ìµœê·¼ ì„±ê³µí•œ Run ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
    url = f"https://api.apify.com/v2/actor-tasks/{task_id}/runs"
    params = {"token": APIFY_TOKEN, "limit": 1, "desc": "true"} # ìµœì‹  1ê°œë§Œ
    try:
        resp = SESSION.get(url, params=params, timeout=timeout)
        resp.raise_for_status() # HTTP ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
        data = resp.json()
        runs = data.get("data", {}).get("items", [])
    except requests.RequestException as e:
        logger.error(f"  âŒ Error fetching runs for task {task_id}: {e}")
        return None
    except json.JSONDecodeError:
        logger.warning(f"  âš ï¸ Invalid JSON response for task {task_id} runs")
        return None

    if not runs:
        logger.warning(f"  âš ï¸ No recent run found for task {task_id}")
        return None

    latest_run = runs[0]
    status = latest_run.get("status", "UNKNOWN")
    # ì„±ê³µí•œ Runë§Œ ë°˜í™˜
    if status == "SUCCEEDED":
        return latest_run
    else:
        run_id = latest_run.get("id", "N/A")
        logger.warning(f"  âš ï¸ Latest run {run_id} for task {task_id} status: {status}. Skipping.")
        return None

def fetch_dataset_items(dataset_id: str, timeout=300):
    """Apify Datasetì˜ ëª¨ë“  ì•„ì´í…œ ê°€ì ¸ì˜¤ê¸° (í˜ì´ì§• ì²˜ë¦¬)"""
    url = f"https://api.apify.com/v2/datasets/{dataset_id}/items"
    params = {"token": APIFY_TOKEN, "format": "json", "clean": "true", "limit": 1000} # í•œ ë²ˆì— 1000ê°œì”©
    all_items = []
    offset = 0
    max_items_limit = 5000 # ìµœëŒ€ 5000ê°œ ì•„ì´í…œ ì œí•œ (ë„ˆë¬´ ë§ì€ ë°ì´í„° ë°©ì§€)

    while True:
        try:
            current_params = params.copy()
            current_params["offset"] = offset
            logger.info(f"  Fetching items... offset={offset}, limit={current_params.get('limit')}")

            resp = SESSION.get(url, params=current_params, timeout=timeout)
            resp.raise_for_status()
            items_data = resp.json() # JSON íŒŒì‹±

            # ì‘ë‹µ ë°ì´í„° í˜•ì‹ í™•ì¸ (ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ë”•ì…”ë„ˆë¦¬)
            current_batch = []
            if isinstance(items_data, list):
                current_batch = items_data
            elif isinstance(items_data, dict) and 'items' in items_data:
                # Apify ìµœì‹  APIëŠ” 'items' í‚¤ë¥¼ í¬í•¨í•  ìˆ˜ ìˆìŒ
                current_batch = items_data['items']
                if not isinstance(current_batch, list):
                    logger.warning(f"  âš ï¸ 'items' key found but not a list in dataset {dataset_id}")
                    break
            else:
                 logger.warning(f"  âš ï¸ Unexpected data format from dataset {dataset_id}: {type(items_data)}")
                 break

            if not current_batch:
                 logger.info("  No more items found in this batch.")
                 break # ë” ì´ìƒ ì•„ì´í…œì´ ì—†ìœ¼ë©´ ì¢…ë£Œ

            all_items.extend(current_batch)
            logger.info(f"  Fetched {len(current_batch)} items. Total now: {len(all_items)}")

            # ë‹¤ìŒ í˜ì´ì§€ ì˜¤í”„ì…‹ ì„¤ì •
            offset += len(current_batch) # ì‹¤ì œ ê°€ì ¸ì˜¨ ê°œìˆ˜ë§Œí¼ ì¦ê°€

            # ìµœëŒ€ ì•„ì´í…œ ì œí•œ í™•ì¸
            if len(all_items) >= max_items_limit:
                 logger.warning(f"  âš ï¸ Reached max items limit ({max_items_limit}). Stopping fetch.")
                 break

            # API í˜¸ì¶œ ê°„ ì§§ì€ ì§€ì—° (Rate limit ë°©ì§€)
            time.sleep(0.5)

        except requests.RequestException as e:
            logger.error(f"  âš ï¸ Items fetch error for dataset {dataset_id} at offset {offset}: {e}")
            break # ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì‹œ ì¤‘ë‹¨
        except json.JSONDecodeError:
            logger.error(f"  âš ï¸ Items JSON decode error for dataset {dataset_id} at offset {offset}")
            break # JSON íŒŒì‹± ì˜¤ë¥˜ ì‹œ ì¤‘ë‹¨
        except Exception as e:
            logger.error(f"  âš ï¸ Unexpected error fetching items for dataset {dataset_id}: {e}")
            break # ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ì‹œ ì¤‘ë‹¨

    logger.info(f"  Total items fetched for dataset {dataset_id}: {len(all_items)}")
    return all_items


# --- ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (run) ---
def run():
    total_upserted = 0
    total_skipped = 0
    total_ai_batches = 0 # AI ë°°ì¹˜ í˜¸ì¶œ íšŸìˆ˜

    print(f"ğŸ¤– AI_IN_PIPELINE: {AI_IN_PIPELINE} (Title-based Hashtag Classification - Batch API calls)")
    print(f"â±ï¸ AI_SLEEP_SEC (between batches): {AI_SLEEP_SEC}")
    print(f"ğŸ”¢ AI_BATCH_SIZE: {AI_BATCH_SIZE}")

    conn = None
    try:
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        conn = psycopg2.connect(DATABASE_URL)
        conn.autocommit = False # íŠ¸ëœì­ì…˜ ê´€ë¦¬ë¥¼ ìœ„í•´ autocommit ë¹„í™œì„±í™”

        # RealDictCursor ì‚¬ìš©: ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°›ìŒ
        with conn.cursor(cursor_factory=RealDictCursor) as cur:
            # colleges.pyì— ì •ì˜ëœ ê° ëŒ€í•™ë³„ë¡œ ì²˜ë¦¬
            for ck, meta in COLLEGES.items():
                college_name = meta.get("name", "Unknown College")
                task_id = meta.get("task_id")
                site = meta.get("url") # ëŒ€í•™ë³„ ê¸°ë³¸ URL

                # task_idê°€ ì—†ìœ¼ë©´ í•´ë‹¹ ëŒ€í•™ ê±´ë„ˆë›°ê¸°
                if not task_id:
                    logger.warning(f"Skipping college {college_name} ({ck}) due to missing task_id.")
                    continue

                print(f"\nğŸ” Processing college: {college_name} ({ck})")

                # ê°€ì¥ ìµœê·¼ ì„±ê³µí•œ Apify Run ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                run_data = get_latest_run_for_task(task_id)
                if not run_data:
                    # ìµœê·¼ ì„±ê³µ Runì´ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                    continue

                run_id = run_data.get("id")
                ds_id = run_data.get("defaultDatasetId")
                finished_at_str = run_data.get("finishedAt", "unknown time")

                # Run ì™„ë£Œ ì‹œê°„ í‘œì‹œ (íŒŒì‹± ì‹œë„)
                try:
                     finished_at_dt = datetime.fromisoformat(finished_at_str.replace("Z", "+00:00"))
                     finished_at_display = finished_at_dt.strftime('%Y-%m-%d %H:%M:%S %Z')
                except:
                     finished_at_display = finished_at_str

                if not ds_id:
                    logger.error(f"  âŒ No datasetId found for the latest successful run {run_id}")
                    continue

                logger.info(f"  ğŸ“… Using data from run {run_id} (Finished: {finished_at_display})")

                # ë°ì´í„°ì…‹ ì•„ì´í…œ ê°€ì ¸ì˜¤ê¸°
                items = fetch_dataset_items(ds_id)
                if not items:
                     logger.warning(f"  âš ï¸ No items fetched from dataset {ds_id}. Skipping college.")
                     continue
                logger.info(f"  ğŸ“¦ Total items retrieved: {len(items)}")

                college_upserted = 0
                college_skipped = 0
                ai_call_count_batch = 0 # í˜„ì¬ ëŒ€í•™ì˜ AI ë°°ì¹˜ í˜¸ì¶œ ìˆ˜
                processed_items_data = [] # ì •ê·œí™” ë° ìœ íš¨ì„± ê²€ì‚¬ í†µê³¼í•œ ì•„ì´í…œ ì €ì¥
                items_to_process_ai = [] # AI ì²˜ë¦¬ê°€ í•„ìš”í•œ ì•„ì´í…œ ì •ë³´ ì €ì¥

                # --- 1ë‹¨ê³„: ì•„ì´í…œ ì •ê·œí™”, ìœ íš¨ì„± ê²€ì‚¬, í•´ì‹œ ìƒì„± ë° AI ì²˜ë¦¬ ëŒ€ìƒ ì„ ë³„ ---
                logger.info("  Preprocessing items...")
                processed_hashes_in_run = set() # í˜„ì¬ Run ë‚´ì—ì„œ ì¤‘ë³µ í•´ì‹œ ë°©ì§€

                for item_index, rec in enumerate(items):
                    # ì•„ì´í…œ ì •ê·œí™” ì‹œë„
                    try:
                        norm = normalize_item(rec, base_url=site)
                    except Exception as norm_err:
                        logger.error(f"  âŒ Error normalizing item {item_index+1}: {norm_err}")
                        college_skipped += 1
                        continue

                    # ìœ íš¨ì„± ê²€ì‚¬
                    if not validate_normalized_item(norm):
                        college_skipped += 1
                        continue

                    # ì½˜í…ì¸  í•´ì‹œ ìƒì„± ë° ì¤‘ë³µ í™•ì¸
                    try:
                        h = content_hash(ck, norm["title"], norm["url"], norm.get("published_at"))
                        # í˜„ì¬ Run ë‚´ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ëœ í•´ì‹œë©´ ê±´ë„ˆë›°ê¸°
                        if h in processed_hashes_in_run:
                             logger.debug(f"  âš ï¸ Skipping duplicate hash within this run: {norm['title'][:30]}...")
                             college_skipped += 1
                             continue
                        processed_hashes_in_run.add(h) # ì²˜ë¦¬ëœ í•´ì‹œë¡œ ì¶”ê°€

                        # DB ì €ì¥ì„ ìœ„í•´ í•„ìš”í•œ ì •ë³´ ì¶”ê°€
                        norm['hash'] = h
                        norm['college_key'] = ck
                        norm['source_site'] = site # source_site ì¶”ê°€
                        processed_items_data.append(norm)

                        # AI ì²˜ë¦¬ ëŒ€ìƒ ì„ ë³„ (AI_IN_PIPELINE í™œì„±í™” ë° ì œëª© ì¡´ì¬ ì‹œ)
                        if AI_IN_PIPELINE and norm.get("title", "").strip():
                            items_to_process_ai.append({
                                "id": h, # í•´ì‹œê°’ì„ AI ê²°ê³¼ ë§¤í•‘ìš© IDë¡œ ì‚¬ìš©
                                "title": norm["title"],
                                "college_name": college_name # ë‹¨ê³¼ëŒ€ ì´ë¦„ë„ AI ì»¨í…ìŠ¤íŠ¸ë¡œ ì œê³µ
                            })
                    except Exception as hash_err:
                        logger.error(f"  âŒ Error generating hash for item {item_index+1} ('{norm.get('title', 'N/A')[:30]}...'): {hash_err}")
                        college_skipped += 1
                        continue

                logger.info(f"  Preprocessing done. Valid items: {len(processed_items_data)}, AI targets: {len(items_to_process_ai)}")

                # --- 2ë‹¨ê³„: AI ë°°ì¹˜ ì²˜ë¦¬ ---
                ai_results_map = {} # { "hash_id": ["#íƒœê·¸1", "#íƒœê·¸2"], ... } í˜•íƒœë¡œ ê²°ê³¼ ì €ì¥
                if AI_IN_PIPELINE and items_to_process_ai:
                    logger.info(f"  Starting AI batch classification (Batch size: {AI_BATCH_SIZE})...")
                    num_batches = (len(items_to_process_ai) + AI_BATCH_SIZE - 1) // AI_BATCH_SIZE
                    total_ai_batches += num_batches # ì „ì²´ ë°°ì¹˜ ìˆ˜ ëˆ„ì 

                    for i in range(num_batches):
                        batch_start_index = i * AI_BATCH_SIZE
                        batch_end_index = batch_start_index + AI_BATCH_SIZE
                        current_batch_input = items_to_process_ai[batch_start_index:batch_end_index]

                        if not current_batch_input:
                            continue

                        logger.info(f"  Processing AI Batch {i+1}/{num_batches} ({len(current_batch_input)} items)...")
                        ai_call_count_batch += 1 # í˜„ì¬ ëŒ€í•™ ë°°ì¹˜ í˜¸ì¶œ ìˆ˜ ì¦ê°€

                        # --- API í˜¸ì¶œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨) ---
                        retry_count = 0
                        max_retries = 2 # ì¬ì‹œë„ íšŸìˆ˜ ì¦ê°€ (ìµœëŒ€ 2ë²ˆ)
                        batch_success = False
                        while retry_count <= max_retries:
                            try:
                                # ì²« ë°°ì¹˜ê°€ ì•„ë‹ˆê±°ë‚˜ ì¬ì‹œë„ ì‹œ ì§€ì—°
                                if i > 0 or retry_count > 0:
                                    sleep_duration = AI_SLEEP_SEC * (retry_count + 1) # ì¬ì‹œë„ ì‹œ ë” ê¸¸ê²Œ ëŒ€ê¸°
                                    logger.debug(f"    Sleeping for {sleep_duration:.1f}s before AI call...")
                                    time.sleep(sleep_duration)

                                # ë°°ì¹˜ ë¶„ë¥˜ í•¨ìˆ˜ í˜¸ì¶œ
                                batch_result = classify_hashtags_from_title_batch(current_batch_input)
                                ai_results_map.update(batch_result) # ê²°ê³¼ ë§µì— ì¶”ê°€
                                batch_success = True
                                logger.info(f"  Batch {i+1} completed successfully.")
                                break # ì„±ê³µ ì‹œ ì¬ì‹œë„ ë£¨í”„ íƒˆì¶œ

                            except Exception as e:
                                # Rate limit ì˜¤ë¥˜ (HTTP 429) ì²˜ë¦¬
                                if "429" in str(e) or "rate limit" in str(e).lower():
                                    retry_count += 1
                                    if retry_count <= max_retries:
                                        wait_time = (2 ** retry_count) * 5 # Exponential backoff (5s, 10s, 20s)
                                        logger.warning(f"  âš ï¸ Rate limit on Batch {i+1}. Retrying in {wait_time}s... ({retry_count}/{max_retries})")
                                        time.sleep(wait_time)
                                    else:
                                         logger.error(f"  âŒ Max retries ({max_retries}) reached for Batch {i+1} due to rate limit. Skipping AI for this batch.")
                                         break # ìµœëŒ€ ì¬ì‹œë„ ë„ë‹¬ ì‹œ í¬ê¸°
                                else:
                                    # ê·¸ ì™¸ AI ì˜¤ë¥˜
                                    logger.error(f"  âŒ AI batch classification failed for Batch {i+1}: {e}. Skipping AI for this batch.")
                                    break # ë³µêµ¬ ë¶ˆê°€ëŠ¥ ì˜¤ë¥˜ ì‹œ í¬ê¸°

                        # --- ì¬ì‹œë„ ë¡œì§ ì¢…ë£Œ ---
                        if not batch_success:
                            # ë°°ì¹˜ ì²˜ë¦¬ì— ì‹¤íŒ¨í•œ ì•„ì´í…œë“¤ì— ëŒ€í•´ ë¹ˆ ë¦¬ìŠ¤íŠ¸([]) ê²°ê³¼ ì„¤ì • (DB ì˜¤ë¥˜ ë°©ì§€)
                            for item_info in current_batch_input:
                                if item_info['id'] not in ai_results_map:
                                    ai_results_map[item_info['id']] = []


                logger.info("  AI processing finished.")

                # --- 3ë‹¨ê³„: DB ì €ì¥ ë£¨í”„ (ì˜¤ë¥˜ ìˆ˜ì • ë° ë¡œê¹… ê°•í™”) ---
                logger.info("  Upserting data into database...")
                for norm_item in processed_items_data:
                    item_hash = norm_item.get('hash')
                    # í•´ì‹œ ì—†ìœ¼ë©´ ì²˜ë¦¬ ë¶ˆê°€
                    if not item_hash:
                        logger.warning(f"  âš ï¸ Skipping item due to missing hash (should not happen): {norm_item.get('title', 'N/A')[:30]}...")
                        college_skipped += 1
                        continue

                    # AI ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° (ê¸°ë³¸ê°’ ì„¤ì • ê°•í™”)
                    hashtags_ai = ai_results_map.get(item_hash, []) # ê¸°ë³¸ê°’ ë¹ˆ ë¦¬ìŠ¤íŠ¸
                    # ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ê°•ì œ ë³€í™˜
                    if not isinstance(hashtags_ai, list):
                        logger.warning(f"  âš ï¸ Hashtags for {item_hash} is not a list ({type(hashtags_ai)}), forcing to []. AI Map: {ai_results_map.get(item_hash)}")
                        hashtags_ai = []

                    # ì¹´í…Œê³ ë¦¬ ì„¤ì • (í•´ì‹œíƒœê·¸ ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜)
                    category_ai = hashtags_ai[0] if hashtags_ai and hashtags_ai != ["#ì¼ë°˜"] else None # #ì¼ë°˜ íƒœê·¸ë§Œ ìˆìœ¼ë©´ ì¹´í…Œê³ ë¦¬ëŠ” None

                    # ê¸°íƒ€ AI í•„ë“œ (í˜„ì¬ ë¡œì§ì—ì„œëŠ” None ë˜ëŠ” ë¹ˆ dict)
                    start_at_ai = None
                    end_at_ai = None
                    # qualification_ai ì²˜ë¦¬ (í•­ìƒ dict ë³´ì¥)
                    # í˜„ì¬ AI ë°°ì¹˜ ê²°ê³¼ì—ëŠ” qualification_aiê°€ ì—†ìœ¼ë¯€ë¡œ ë¹ˆ dict ì‚¬ìš©
                    raw_qualification_ai = {} # <<-- ì´ ë¶€ë¶„ì€ ë‚˜ì¤‘ì— ìê²©ìš”ê±´ ì¶”ì¶œ ë¡œì§ ì¶”ê°€ ì‹œ ìˆ˜ì • í•„ìš”
                    if not isinstance(raw_qualification_ai, dict):
                        logger.warning(f"  âš ï¸ Qualification AI result for {item_hash} was not a dict (type: {type(raw_qualification_ai)}), using empty dict.")
                        qualification_ai = {}
                    else:
                        qualification_ai = raw_qualification_ai

                    # DB ì €ì¥ ì‹œë„ (try-except ë¸”ë¡ ê°•í™”)
                    try:
                        cur.execute(UPSERT_SQL, {
                            "college_key": norm_item.get('college_key'), # college_key í™•ì¸
                            "title": norm_item.get("title"),
                            "url": norm_item.get("url"),
                            "summary_raw": norm_item.get("summary_raw"),
                            "body_html": norm_item.get("body_html"),
                            "body_text": norm_item.get("body_text"),
                            "published_at": norm_item.get("published_at"),
                            "source_site": norm_item.get('source_site'), # source_site í™•ì¸
                            "content_hash": item_hash,
                            "category_ai": category_ai,
                            "start_at_ai": start_at_ai,
                            "end_at_ai": end_at_ai,
                            "qualification_ai": Json(qualification_ai), # Json() ì‚¬ìš© (ì´ì œ qualification_aiëŠ” dict)
                            "hashtags_ai": hashtags_ai, # ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸
                        })
                        # cur.rowcount > 0 ì´ë©´ ì‹¤ì œë¡œ INSERT ë˜ëŠ” UPDATE ë°œìƒ
                        # logger.debug(f"Upsert executed for hash {item_hash}. Row count: {cur.rowcount}")
                        college_upserted += 1 # ì‹¤í–‰ ìì²´ë¥¼ ì„±ê³µìœ¼ë¡œ ì¹´ìš´íŠ¸ (ON CONFLICT DO UPDATEë„ í¬í•¨)

                    except psycopg2.Error as db_err:
                        conn.rollback() # í˜„ì¬ ì•„ì´í…œ ë¡¤ë°± (íŠ¸ëœì­ì…˜ ìœ ì§€)
                        # ìƒì„¸í•œ DB ì˜¤ë¥˜ ë¡œê·¸ ì¶œë ¥
                        pgcode = getattr(db_err, 'pgcode', 'N/A')
                        pgerror = getattr(db_err, 'pgerror', str(db_err)).strip()
                        diag = getattr(db_err, 'diag', None)
                        diag_message = diag.message_detail if diag and hasattr(diag, 'message_detail') else pgerror

                        logger.error(f"  âŒ DB error upserting '{norm_item.get('title', 'N/A')[:30]}...' (Hash: {item_hash}):")
                        logger.error(f"     Code: {pgcode}, Detail: {diag_message}")
                        college_skipped += 1
                        # ì—¬ê¸°ì„œ continue ë˜ëŠ” break ê²°ì • ê°€ëŠ¥ (ì¼ë‹¨ ê³„ì† ì§„í–‰)
                    except Exception as general_err:
                         conn.rollback() # í˜„ì¬ ì•„ì´í…œ ë¡¤ë°±
                         logger.error(f"  âŒ Unexpected error during upsert for '{norm_item.get('title', 'N/A')[:30]}...': {general_err}")
                         college_skipped += 1
                         # ì—¬ê¸°ì„œ continue ë˜ëŠ” break ê²°ì • ê°€ëŠ¥ (ì¼ë‹¨ ê³„ì† ì§„í–‰)

                # --- DB ì €ì¥ ë£¨í”„ ì¢…ë£Œ ---

                # í•œ ëŒ€í•™ ì²˜ë¦¬ í›„ ì»¤ë°‹ (ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¡¤ë°±ë˜ì—ˆìœ¼ë¯€ë¡œ ì„±ê³µí•œ ê²ƒë§Œ ì»¤ë°‹ë¨)
                conn.commit()
                logger.info(f"  âœ… Finished {college_name}: Upserted attempts={college_upserted}, Skipped={college_skipped}, AI Batches={ai_call_count_batch}")

                total_upserted += college_upserted
                total_skipped += college_skipped

            # --- ëª¨ë“  ëŒ€í•™ ì²˜ë¦¬ ë£¨í”„ ì¢…ë£Œ ---

    except psycopg2.Error as db_conn_err:
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìì²´ì˜ ë¬¸ì œ
        logger.critical(f"\nâŒ Database connection error: {db_conn_err}")
        # ì´ ê²½ìš° ì¶”ê°€ ì²˜ë¦¬ê°€ ì–´ë ¤ìš°ë¯€ë¡œ ì¢…ë£Œ
    except KeyboardInterrupt:
        # ì‚¬ìš©ìê°€ Ctrl+C ë“±ìœ¼ë¡œ ì¤‘ë‹¨ ì‹œ
        logger.warning("\nğŸš« Operation cancelled by user.")
        if conn:
            conn.rollback() # ì§„í–‰ ì¤‘ì´ë˜ íŠ¸ëœì­ì…˜ ë¡¤ë°±
    except Exception as e:
        # ê·¸ ì™¸ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜
        logger.exception(f"\nâŒ An unexpected error occurred during the run: {e}") # ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ í¬í•¨
        if conn:
            conn.rollback() # ì§„í–‰ ì¤‘ì´ë˜ íŠ¸ëœì­ì…˜ ë¡¤ë°±
    finally:
        # í•­ìƒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ
        if conn:
            conn.close()
            logger.info("Database connection closed.")

    # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    print(f"\nâœ¨ Script finished.")
    print(f"Total upsert attempts: {total_upserted}")
    print(f"Total skipped items: {total_skipped}")
    print(f"Total AI batch calls: {total_ai_batches}")

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹œì‘ì 
if __name__ == "__main__":
    run()