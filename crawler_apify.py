# crawler_apify.py (ê°œì„ ëœ normalize ë¡œì§)
import os, time, json, hashlib, requests, psycopg2
import re
from datetime import datetime, timezone
from urllib.parse import urlencode, urlparse, urljoin
from psycopg2.extras import RealDictCursor
from dotenv import load_dotenv
from typing import Optional, Dict, Any, List
from html import unescape
from bs4 import BeautifulSoup
from colleges import COLLEGES

load_dotenv(encoding="utf-8")

APIFY_TOKEN = os.getenv("APIFY_TOKEN")
DATABASE_URL = os.getenv("DATABASE_URL")

if not APIFY_TOKEN:
    raise RuntimeError("APIFY_TOKEN not set")
if not DATABASE_URL:
    raise RuntimeError("DATABASE_URL not set")

SESSION = requests.Session()
SESSION.headers.update({"Accept": "application/json"})

UPSERT_SQL = """
INSERT INTO notices (
  college_key, title, url, summary_raw, body_html, body_text, published_at, source_site, content_hash
) VALUES (
  %(college_key)s, %(title)s, %(url)s, %(summary_raw)s, %(body_html)s, %(body_text)s, %(published_at)s, %(source_site)s, %(content_hash)s
)
ON CONFLICT (content_hash) DO UPDATE
SET
  title = EXCLUDED.title,
  url = EXCLUDED.url,
  summary_raw = EXCLUDED.summary_raw,
  body_html = EXCLUDED.body_html,
  body_text = EXCLUDED.body_text,
  published_at = COALESCE(EXCLUDED.published_at, notices.published_at),
  source_site = EXCLUDED.source_site,
  updated_at = CURRENT_TIMESTAMP;
"""

def clean_text(text: Optional[str], max_length: Optional[int] = None) -> str:
    """í…ìŠ¤íŠ¸ ì •ë¦¬ ë° ì •ê·œí™”"""
    if not text:
        return ""
    
    # HTML ì—”í‹°í‹° ë””ì½”ë”©
    text = unescape(text)
    
    # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ í†µí•©
    text = re.sub(r'\s+', ' ', text)
    
    # ì•ë’¤ ê³µë°± ì œê±°
    text = text.strip()
    
    # ê¸¸ì´ ì œí•œ (í•„ìš”ì‹œ)
    if max_length and len(text) > max_length:
        text = text[:max_length-3] + "..."
    
    return text

def extract_text_from_html(html: Optional[str]) -> str:
    """HTMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    if not html:
        return ""
    
    try:
        soup = BeautifulSoup(html, 'html.parser')
        
        # script, style íƒœê·¸ ì œê±°
        for tag in soup(['script', 'style', 'meta', 'link']):
            tag.decompose()
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = soup.get_text(separator=' ', strip=True)
        return clean_text(text)
    except Exception as e:
        print(f"  âš ï¸ HTML parsing error: {e}")
        return ""

def normalize_url(url: Optional[str], base_url: Optional[str] = None) -> str:
    """URL ì •ê·œí™” ë° ì ˆëŒ€ ê²½ë¡œ ë³€í™˜"""
    if not url:
        return ""
    
    url = url.strip()
    
    # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° base_urlê³¼ ê²°í•©
    if base_url and not url.startswith(('http://', 'https://', '//')):
        url = urljoin(base_url, url)
    
    # // ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš° https:// ì¶”ê°€
    if url.startswith('//'):
        url = 'https:' + url
    
    # URL ìœ íš¨ì„± ê¸°ë³¸ ê²€ì‚¬
    parsed = urlparse(url)
    if not parsed.scheme or not parsed.netloc:
        return ""
    
    return url

def parse_dt(v: Any) -> Optional[datetime]:
    """ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë‚ ì§œ/ì‹œê°„ íŒŒì‹± (ê°œì„ ëœ ë²„ì „)"""
    if not v:
        return None
    
    # datetime ê°ì²´ì¸ ê²½ìš°
    if isinstance(v, datetime):
        return v if v.tzinfo else v.replace(tzinfo=timezone.utc)
    
    # ìˆ«ì íƒ€ì„ìŠ¤íƒ¬í”„ì¸ ê²½ìš°
    if isinstance(v, (int, float)):
        try:
            # ë°€ë¦¬ì´ˆ íƒ€ì„ìŠ¤íƒ¬í”„ ì²˜ë¦¬
            ts = v / 1000 if v > 10_000_000_000 else v
            return datetime.fromtimestamp(ts, tz=timezone.utc)
        except (ValueError, OSError):
            return None
    
    # ë¬¸ìì—´ì¸ ê²½ìš°
    if isinstance(v, str):
        v = v.strip()
        if not v:
            return None
        
        # ISO í˜•ì‹ ì²˜ë¦¬
        v = v.replace("Z", "+00:00")
        try:
            dt = datetime.fromisoformat(v)
            return dt if dt.tzinfo else dt.replace(tzinfo=timezone.utc)
        except ValueError:
            pass
        
        # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì‹œë„
        date_formats = [
            "%Y-%m-%d %H:%M:%S",
            "%Y-%m-%d %H:%M",
            "%Y-%m-%d",
            "%Y/%m/%d %H:%M:%S",
            "%Y/%m/%d %H:%M",
            "%Y/%m/%d",
            "%Y.%m.%d %H:%M:%S",
            "%Y.%m.%d %H:%M",
            "%Y.%m.%d",
            "%d/%m/%Y %H:%M:%S",
            "%d/%m/%Y",
            "%d-%m-%Y",
            "%Yë…„ %mì›” %dì¼ %H:%M",
            "%Yë…„ %mì›” %dì¼",
        ]
        
        for fmt in date_formats:
            try:
                dt = datetime.strptime(v, fmt)
                return dt.replace(tzinfo=timezone.utc)
            except ValueError:
                continue
    
    return None

def extract_field(item: Dict[str, Any], field_names: List[str], 
                  default: str = "") -> Optional[str]:
    """ì—¬ëŸ¬ ê°€ëŠ¥í•œ í•„ë“œëª…ì—ì„œ ê°’ ì¶”ì¶œ"""
    for field in field_names:
        # ì¤‘ì²©ëœ í•„ë“œ ì²˜ë¦¬ (ì˜ˆ: "meta.title")
        if '.' in field:
            parts = field.split('.')
            value = item
            for part in parts:
                if isinstance(value, dict):
                    value = value.get(part)
                else:
                    value = None
                    break
            if value:
                return value
        else:
            value = item.get(field)
            if value:
                return value
    return default

def normalize_item(item: dict, base_url: Optional[str] = None) -> dict:
    """ì•„ì´í…œ ì •ê·œí™” (ê°œì„ ëœ ë²„ì „)"""
    
    # ì œëª© ì¶”ì¶œ (ë” ë§ì€ í•„ë“œ í™•ì¸)
    title_fields = [
        "title", "name", "subject", "headline", 
        "meta.title", "og:title", "titleText"
    ]
    title = clean_text(extract_field(item, title_fields), max_length=500)
    
    # URL ì¶”ì¶œ ë° ì •ê·œí™”
    url_fields = [
        "url", "link", "href", "permalink", 
        "canonical", "meta.url", "og:url"
    ]
    url = normalize_url(extract_field(item, url_fields), base_url)
    
    # ìš”ì•½ ì¶”ì¶œ
    summary_fields = [
        "summary", "description", "excerpt", "preview",
        "meta.description", "og:description", "abstract"
    ]
    summary_raw = extract_field(item, summary_fields)
    if summary_raw:
        summary_raw = clean_text(summary_raw, max_length=1000)
    
    # HTML ë³¸ë¬¸ ì²˜ë¦¬
    html_fields = ["html", "content_html", "body_html", "htmlContent"]
    body_html = extract_field(item, html_fields)
    
    # í…ìŠ¤íŠ¸ ë³¸ë¬¸ ì²˜ë¦¬
    text_fields = ["text", "content", "body", "body_text", "plainText"]
    body_text = extract_field(item, text_fields)
    
    # HTMLì´ ìˆì§€ë§Œ í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ê²½ìš°, HTMLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    if body_html and not body_text:
        body_text = extract_text_from_html(body_html)
    
    # í…ìŠ¤íŠ¸ê°€ ìˆì§€ë§Œ ìš”ì•½ì´ ì—†ëŠ” ê²½ìš°, í…ìŠ¤íŠ¸ì—ì„œ ìš”ì•½ ìƒì„±
    if body_text and not summary_raw:
        summary_raw = clean_text(body_text[:500])
    
    # ë‚ ì§œ ì¶”ì¶œ (ë” ë§ì€ í•„ë“œ í™•ì¸)
    date_fields = [
        "publishedAt", "published_at", "pubDate", "date", 
        "datetime", "time", "createdAt", "created_at",
        "timestamp", "postDate", "releaseDate"
    ]
    published_at = None
    for field in date_fields:
        value = item.get(field)
        if value:
            published_at = parse_dt(value)
            if published_at:
                break
    
    # ì¹´í…Œê³ ë¦¬ë‚˜ íƒœê·¸ ì •ë³´ ì¶”ì¶œ (ì„ íƒì )
    category_fields = ["category", "categories", "tag", "tags", "section"]
    category = extract_field(item, category_fields)
    
    # ì‘ì„±ì ì •ë³´ ì¶”ì¶œ (ì„ íƒì )
    author_fields = ["author", "writer", "creator", "by"]
    author = extract_field(item, author_fields)
    
    result = {
        "title": title,
        "url": url,
        "summary_raw": summary_raw,
        "body_html": body_html,
        "body_text": body_text,
        "published_at": published_at,
    }
    
    # ì¶”ê°€ ë©”íƒ€ë°ì´í„° (í•„ìš”ì‹œ ì‚¬ìš©)
    if category:
        result["category"] = clean_text(str(category))
    if author:
        result["author"] = clean_text(str(author))
    
    return result

def validate_normalized_item(item: dict) -> bool:
    """ì •ê·œí™”ëœ ì•„ì´í…œì˜ ìœ íš¨ì„± ê²€ì¦"""
    # í•„ìˆ˜ í•„ë“œ í™•ì¸
    if not item.get("title") or not item.get("url"):
        return False
    
    # URL ìœ íš¨ì„± í™•ì¸
    if not item["url"].startswith(('http://', 'https://')):
        return False
    
    # ì œëª© ìµœì†Œ ê¸¸ì´ í™•ì¸
    if len(item["title"]) < 3:
        return False
    
    # ë‚ ì§œê°€ ë¯¸ë˜ê°€ ì•„ë‹Œì§€ í™•ì¸
    if item.get("published_at"):
        if item["published_at"] > datetime.now(timezone.utc):
            return False
    
    return True

def content_hash(college_key: str, title: str, url: str, 
                published_at: Optional[datetime]) -> str:
    """ì»¨í…ì¸  í•´ì‹œ ìƒì„± (ê°œì„ ëœ ë²„ì „)"""
    # URL ì •ê·œí™” (trailing slash ì œê±° ë“±)
    url = url.rstrip('/')
    
    # ë‚ ì§œëŠ” ì¼ ë‹¨ìœ„ë¡œë§Œ ì‚¬ìš© (ì‹œê°„ ë¬´ì‹œ)
    date_str = ""
    if published_at:
        date_str = published_at.date().isoformat()
    
    # ì œëª© ì •ê·œí™” (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ, ê³µë°± ì •ë¦¬)
    title_normalized = re.sub(r'\s+', ' ', title.lower().strip())
    
    base = f"{college_key}|{title_normalized}|{url}|{date_str}"
    return hashlib.sha256(base.encode('utf-8')).hexdigest()

# ---------------- Apify helpers (ê¸°ì¡´ ì½”ë“œ ìœ ì§€) ----------------

def start_task_run(task_id: str, timeout=30):
    """POST /v2/actor-tasks/{taskId}/runs"""
    url = f"https://api.apify.com/v2/actor-tasks/{task_id}/runs"
    params = {"token": APIFY_TOKEN}
    try:
        resp = SESSION.post(url, params=params, timeout=timeout)
    except requests.RequestException as e:
        print(f"  âŒ start run error: {e}")
        return None
    if resp.status_code not in (201, 200):
        print(f"  âŒ start run HTTP {resp.status_code}: {resp.text[:300]}")
        return None
    try:
        data = resp.json()
    except ValueError:
        print("  âš ï¸ start run: empty body")
        return None
    run = data.get("data") or data
    return run.get("id")

def poll_run_until_done(run_id: str, max_wait_sec=600, poll_interval=3):
    """GET /v2/actor-runs/{runId} ìƒíƒœ í´ë§"""
    url = f"https://api.apify.com/v2/actor-runs/{run_id}"
    params = {"token": APIFY_TOKEN}
    waited = 0
    while waited <= max_wait_sec:
        try:
            resp = SESSION.get(url, params=params, timeout=30)
            if resp.status_code != 200:
                print(f"  âš ï¸ poll HTTP {resp.status_code}: {resp.text[:200]}")
            else:
                data = resp.json().get("data") or {}
                status = data.get("status")
                if status in ("SUCCEEDED", "FAILED", "TIMED_OUT", "ABORTED"):
                    return data
        except requests.RequestException as e:
            print(f"  âš ï¸ poll error: {e}")
        time.sleep(poll_interval)
        waited += poll_interval
    print("  âš ï¸ poll timeout")
    return None

def fetch_dataset_items(dataset_id: str, timeout=300):
    """ë°ì´í„°ì…‹ ì•„ì´í…œ ê°€ì ¸ì˜¤ê¸°"""
    url = f"https://api.apify.com/v2/datasets/{dataset_id}/items"
    params = {"token": APIFY_TOKEN, "format": "json", "clean": "true"}
    try:
        resp = SESSION.get(url, params=params, timeout=timeout)
        if resp.status_code == 200:
            data = resp.json()
            return data if isinstance(data, list) else data.get("items", [])
        print(f"  âš ï¸ items HTTP {resp.status_code}: {resp.text[:300]}")
    except requests.RequestException as e:
        print(f"  âš ï¸ items error: {e}")
    return []

# ---------------- main flow (ê°œì„ ëœ ë²„ì „) ----------------

def run():
    total_upserted = 0
    total_skipped = 0
    
    with psycopg2.connect(DATABASE_URL) as conn, conn.cursor(cursor_factory=RealDictCursor) as cur:
        for ck, meta in COLLEGES.items():
            name = meta["name"]
            task_id = meta["task_id"]
            site = meta.get("url")
            
            print(f"ğŸ” Start task: {name} ({ck})")
            
            # íƒœìŠ¤í¬ ì‹¤í–‰
            run_id = start_task_run(task_id)
            if not run_id:
                print(f"  âŒ cannot start run for {ck}")
                continue

            # ì™„ë£Œ ëŒ€ê¸°
            run_data = poll_run_until_done(run_id)
            if not run_data:
                print(f"  âŒ run polling failed for {ck}")
                continue
                
            status = run_data.get("status")
            ds_id = run_data.get("defaultDatasetId")
            
            if status != "SUCCEEDED":
                print(f"  âŒ run status={status} for {ck}")
                continue
            if not ds_id:
                print(f"  âŒ no datasetId for {ck}")
                continue

            # ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            items = fetch_dataset_items(ds_id)
            print(f"  ğŸ“¦ items fetched: {len(items)}")
            
            college_upserted = 0
            college_skipped = 0

            for rec in items:
                # ì •ê·œí™”
                norm = normalize_item(rec, base_url=site)
                
                # ìœ íš¨ì„± ê²€ì¦
                if not validate_normalized_item(norm):
                    college_skipped += 1
                    continue
                
                # í•´ì‹œ ìƒì„±
                h = content_hash(ck, norm["title"], norm["url"], norm["published_at"])
                
                # DB ì €ì¥
                try:
                    cur.execute(UPSERT_SQL, {
                        "college_key": ck,
                        "title": norm["title"],
                        "url": norm["url"],
                        "summary_raw": norm["summary_raw"],
                        "body_html": norm["body_html"],
                        "body_text": norm["body_text"],
                        "published_at": norm["published_at"],
                        "source_site": site,
                        "content_hash": h
                    })
                    college_upserted += 1
                except psycopg2.Error as e:
                    print(f"  âš ï¸ DB error for {norm['title'][:50]}: {e}")
                    college_skipped += 1
            
            conn.commit()
            print(f"  âœ… {name}: upserted={college_upserted}, skipped={college_skipped}")
            
            total_upserted += college_upserted
            total_skipped += college_skipped
    
    print(f"\nâœ¨ Total: upserted={total_upserted}, skipped={total_skipped}")

if __name__ == "__main__":
    run()