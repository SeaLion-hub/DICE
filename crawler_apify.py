# crawler_apify.py (raw_text ì €ì¥ ë° ì •ì œ ë¡œì§ í¬í•¨)
import os
import time
import json
import hashlib
import requests
import psycopg2
import re
import datetime as dt
from datetime import datetime, timezone
from urllib.parse import urlencode, urlparse, urljoin
from psycopg2.extras import RealDictCursor, Json
from dotenv import load_dotenv
from typing import Optional, Dict, Any, List, Tuple
from html import unescape
from bs4 import BeautifulSoup
from colleges import COLLEGES
import logging # ë¡œê¹… ì„í¬íŠ¸ ì¶”ê°€

# AI processor import ìˆ˜ì • (ë°°ì¹˜ ë¶„ë¥˜ í•¨ìˆ˜ ì‚¬ìš©)
from ai_processor import (
    classify_hashtags_from_title_batch,
    extract_structured_info,
    extract_detailed_hashtags,
)
# _to_utc_ts í•¨ìˆ˜ import
try:
    from main import _to_utc_ts
except ImportError:
    print("Warning: Could not import _to_utc_ts from main.py. Defining locally.")
    def _to_utc_ts(date_yyyy_mm_dd: str | None):
        if not date_yyyy_mm_dd:
            return None
        try:
            d = dt.date.fromisoformat(date_yyyy_mm_dd)
            return datetime(d.year, d.month, d.day, tzinfo=timezone.utc)
        except (ValueError, TypeError):
            print(f"Warning: Invalid date format: {date_yyyy_mm_dd}. Returning None.")
            return None

# [ë³€ê²½ ì—†ìŒ] clean_body_text í•¨ìˆ˜ëŠ” ì´ë¯¸ raw_textë¥¼ ë°›ì•„ ì •ì œí•˜ë„ë¡ ë˜ì–´ ìˆìŒ
def clean_body_text(raw_text: str, college_key: Optional[str] = None) -> str:
    """
    Apifyì—ì„œ í¬ë¡¤ë§í•œ ì›ë³¸ body_textì—ì„œ ë¶ˆí•„ìš”í•œ
    CDATA ìŠ¤í¬ë¦½íŠ¸, HTML íƒœê·¸, í—¤ë”, í‘¸í„° ì •ë³´ë¥¼ ì œê±°í•˜ì—¬ ìˆœìˆ˜ ë³¸ë¬¸ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    (ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ "ê²Œì‹œê¸€ ë‚´ìš©"ê³¼ í‘¸í„° ë§ˆì»¤ ì‚¬ì´ë¥¼ ì¶”ì¶œí•˜ë„ë¡ ìˆ˜ì •)
    """
    if not raw_text:
        return ""

    # 1. HTML ì—”í‹°í‹° ë³µì› (e.g., &lt; -> <)
    text = unescape(raw_text)

    # 2. JavaScript CDATA ë¸”ë¡ ì œê±° (ì‚¬ìš©ì ì˜ˆì‹œ íŒ¨í„´)
    text = re.sub(r'//<!\[CDATA\[.*?//\]\]>', '', text, flags=re.DOTALL)

    # 3. BeautifulSoupì„ ì‚¬ìš©í•˜ì—¬ HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
    soup = BeautifulSoup(text, 'html.parser')
    text = soup.get_text(separator='\n', strip=True)

    # 4. í—¤ë”(Header) ì •ë³´ ì œê±°
    #    ì‚¬ìš©ì ìš”ì²­: "ê²Œì‹œê¸€ ë‚´ìš©"ì„ ì‹œì‘ ë§ˆì»¤ë¡œ ì‚¬ìš©
    start_marker = r'ê²Œì‹œê¸€ ë‚´ìš©'
    start_match = re.search(start_marker, text, re.IGNORECASE)
    
    start_index = 0
    if start_match:
        start_index = start_match.end() # "ê²Œì‹œê¸€ ë‚´ìš©" *ì´í›„* ë¶€í„°
    else:
        # "ê²Œì‹œê¸€ ë‚´ìš©"ì´ ì—†ìœ¼ë©´, ê¸°ì¡´ì˜ ë‹¤ë¥¸ í—¤ë” ë§ˆì»¤ë¡œ ëŒ€ì²´ (ì•ˆì „ì¥ì¹˜)
        header_end_patterns = [
            r'ì¡°íšŒìˆ˜\s+\d+',
            # '.xlsx', '.pdf' ë“± ì²¨ë¶€íŒŒì¼ ë§í¬ (ê³µë°±ì´ë‚˜ ì¤„ë°”ê¿ˆìœ¼ë¡œ ëë‚¨)
            r'\.(xlsx|pdf|hwp|doc|docx|zip|jpg|png|jpeg|gif)(\s|\n|$)',
        ]
        last_header_end_index = -1
        for pattern in header_end_patterns:
            matches = list(re.finditer(pattern, text, re.IGNORECASE))
            if matches:
                # ë§ˆì§€ë§‰ ì¼ì¹˜ í•­ëª©ì˜ ë ìœ„ì¹˜ë¥¼ ì°¾ìŒ
                last_match_end = matches[-1].end()
                if last_match_end > last_header_end_index:
                    last_header_end_index = last_match_end

        if last_header_end_index != -1 and last_header_end_index < len(text):
            start_index = last_header_end_index # ë‹¤ë¥¸ í—¤ë” ë§ˆì»¤ ìœ„ì¹˜
        # else: start_indexëŠ” 0 ìœ ì§€ (ì²˜ìŒë¶€í„°)
    
    # "ê²Œì‹œê¸€ ë‚´ìš©" ë§ˆì»¤ë¥¼ ì°¾ì•˜ë“  ëª» ì°¾ì•˜ë“ , start_indexë¶€í„° í…ìŠ¤íŠ¸ë¥¼ ìë¦„
    text = text[start_index:]


    # 5. í‘¸í„°(Footer) ì •ë³´ ì œê±°
    #    ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ ë§ˆì»¤ ìš°ì„ ìˆœìœ„ ë° ê·œì¹™ ë³€ê²½

    # 5a. college_keyì— ë”°ë¥¸ ì˜ˆì™¸ ì²˜ë¦¬ (ì˜ê³¼ëŒ€í•™)
    # (colleges.pyì˜ 'med' í‚¤ë¼ê³  ê°€ì •)
    if college_key == 'med':
        primary_footer_markers = [
            r'ì—°ì„¸ëŒ€í•™êµ ì˜ê³¼ëŒ€í•™ TAG',  # ì˜ëŒ€ ìš°ì„ 
            r'\sTAG\s',               # ì˜ëŒ€ ìš°ì„ 
        ]
    else:
        # 5b. ì¼ë°˜ ê·œì¹™
        primary_footer_markers = [
            r'ëª©ë¡\s+ì´ì „ê¸€' # 'ëª©ë¡ ì´ì „ê¸€', 'ëª©ë¡  ì´ì „ê¸€'
        ]

    # 5c. (Fallback) ê¸°ì¡´ì˜ ë‹¤ë¥¸ í‘¸í„° ë§ˆì»¤ë“¤
    fallback_footer_markers = [
        r'ì—°ì„¸ëŒ€í•™êµ ê´€ë ¨ì‚¬ì´íŠ¸',
        r'COPYRIGHTÂ©',
        r'ì±„ìš©ê³µê³ \s+ì…ì°°ê³µê³ '
    ]
    
    # college_key ì¡°ê±´ì— ë”°ë¼ fallback ë§ˆì»¤ ëª©ë¡ì„ ì¡°ì •
    if college_key == 'med':
        # ì˜ëŒ€ì¸ ê²½ìš°, 'ëª©ë¡ ì´ì „ê¸€'ì„ fallbackì— ì¶”ê°€
        fallback_footer_markers.append(r'ëª©ë¡\s+ì´ì „ê¸€')
    else:
        # ì˜ëŒ€ê°€ ì•„ë‹Œ ê²½ìš°, 'TAG' ê´€ë ¨ì„ fallbackì— ì¶”ê°€
        fallback_footer_markers.extend([r'ì—°ì„¸ëŒ€í•™êµ ì˜ê³¼ëŒ€í•™ TAG', r'\sTAG\s'])

    # 5d. í‘¸í„° íŒ¨í„´ ì»´íŒŒì¼ ë° ê²€ìƒ‰ (ìš°ì„ ìˆœìœ„ ë§ˆì»¤ + fallback ë§ˆì»¤)
    all_footer_markers = primary_footer_markers + fallback_footer_markers
    footer_pattern = re.compile('|'.join(all_footer_markers), re.IGNORECASE | re.DOTALL)
    
    match = footer_pattern.search(text) # (ì´ì œ textëŠ” start_index ì´í›„ì˜ ë‚´ìš©ì„)
    if match:
        # í‘¸í„° ë§ˆì»¤ê°€ ì‹œì‘ë˜ëŠ” ìœ„ì¹˜ì˜ í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©
        text = text[:match.start()]

    # 6. ìµœì¢… ì •ë¦¬: ì•ë’¤ ê³µë°± ë° ë¶ˆí•„ìš”í•œ ê°œí–‰ ë¬¸ì ì •ëˆ
    text = re.sub(r'(\n\s*){3,}', '\n\n', text) # 3ì¤„ ì´ìƒì˜ ê°œí–‰ì„ 2ì¤„ë¡œ
    
    return text.strip()


load_dotenv(encoding="utf-8")

APIFY_TOKEN = os.getenv("APIFY_TOKEN")
DATABASE_URL = os.getenv("DATABASE_URL")
AI_IN_PIPELINE = os.getenv("AI_IN_PIPELINE", "true").lower() == "true"
AI_SLEEP_SEC = float(os.getenv("AI_SLEEP_SEC", "1.0"))
AI_BATCH_SIZE = int(os.getenv("AI_BATCH_SIZE", "10"))
AI_STEP2_SLEEP_SEC = float(os.getenv("AI_STEP2_SLEEP_SEC", str(AI_SLEEP_SEC)))
AI_STEP3_SLEEP_SEC = float(os.getenv("AI_STEP3_SLEEP_SEC", str(AI_SLEEP_SEC)))
AI_MAX_RETRIES = int(os.getenv("AI_MAX_RETRIES", "2"))

if not APIFY_TOKEN:
    raise RuntimeError("APIFY_TOKEN not set")
if not DATABASE_URL:
    raise RuntimeError("DATABASE_URL not set")

# ë¡œê±° ì„¤ì • ì¶”ê°€
logger = logging.getLogger(__name__) # __name__ìœ¼ë¡œ ë¡œê±° ì´ë¦„ ì„¤ì •
logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s")


SESSION = requests.Session()
SESSION.headers.update({"Accept": "application/json"})

# â­ï¸ [ìˆ˜ì •] UPSERT SQL: detailed_hashtags ì»¬ëŸ¼ ì¶”ê°€
UPSERT_SQL = """
INSERT INTO notices (
    college_key, title, url, body_html, body_text, raw_text,
    published_at, source_site, content_hash,
    category_ai, start_at_ai, end_at_ai, qualification_ai, hashtags_ai,
    detailed_hashtags -- [ìˆ˜ì • 1] INSERT ëª©ë¡ì— ì»¬ëŸ¼ ì¶”ê°€
) VALUES (
    %(college_key)s, %(title)s, %(url)s,
    %(body_html)s, %(body_text)s, %(raw_text)s,
    %(published_at)s, %(source_site)s, %(content_hash)s,
    %(category_ai)s, %(start_at_ai)s, %(end_at_ai)s, %(qualification_ai)s, %(hashtags_ai)s,
    %(detailed_hashtags)s -- [ìˆ˜ì • 2] VALUES ëª©ë¡ì— íŒŒë¼ë¯¸í„° ì¶”ê°€
)
ON CONFLICT (content_hash)
DO UPDATE SET
    title = EXCLUDED.title,
    url = EXCLUDED.url,
    body_html = EXCLUDED.body_html,
    body_text = EXCLUDED.body_text,
    raw_text = EXCLUDED.raw_text,
    published_at = EXCLUDED.published_at,
    category_ai = EXCLUDED.category_ai,
    start_at_ai = EXCLUDED.start_at_ai,
    end_at_ai = EXCLUDED.end_at_ai,
    qualification_ai = EXCLUDED.qualification_ai,
    hashtags_ai = EXCLUDED.hashtags_ai,
    detailed_hashtags = EXCLUDED.detailed_hashtags, -- [ìˆ˜ì • 3] ì´ ì°¸ì¡°ê°€ ì´ì œ ìœ íš¨í•¨
    updated_at = CURRENT_TIMESTAMP
RETURNING id;
"""

# --- ê¸°ì¡´ í—¬í¼ í•¨ìˆ˜ë“¤ (clean_text, extract_text_from_html, ...) ---
def clean_text(text: Optional[str], max_length: Optional[int] = None) -> str:
    """í…ìŠ¤íŠ¸ ì •ë¦¬ (ì¤„ë°”ê¿ˆì€ ìœ ì§€í•˜ë„ë¡ ìˆ˜ì •)"""
    if not text: return ""
    text = unescape(text) # HTML ì—”í‹°í‹° ë””ì½”ë”©
    
    # ì—¬ëŸ¬ ì¤„ì˜ ê³µë°±/ê°œí–‰ì„ ìµœëŒ€ 2ì¤„ë¡œ
    text = re.sub(r'(\n\s*){3,}', '\n\n', text)
    # ì¼ë°˜ì ì¸ ì—°ì† ê³µë°± (ì¤„ë°”ê¿ˆ ì œì™¸)ì„ í•˜ë‚˜ë¡œ
    text = re.sub(r'[ \t\r\f\v]+', ' ', text)
    
    text = text.strip() # ì•ë’¤ ê³µë°± ì œê±°
    if max_length and len(text) > max_length: # ê¸¸ì´ ì œí•œ
        text = text[:max_length-3] + "..."
    return text

def extract_text_from_html(html: Optional[str]) -> str:
    """HTMLì—ì„œ ì£¼ìš” í…ìŠ¤íŠ¸ ë‚´ìš©ë§Œ ì¶”ì¶œ ì‹œë„ (ì˜¤ë¥˜ë‚˜ëŠ” ëŒ€í•™ íŒ¨í„´ ìˆ˜ì •)"""
    if not html: return ""
    try:
        # 1. CDATA ìŠ¤í¬ë¦½íŠ¸ ë¨¼ì € ì œê±° (íŒŒì‹± ì˜¤ë¥˜ ë°©ì§€)
        text_content = re.sub(r'//<!\[CDATA\[.*?//\]\]>', '', html, flags=re.DOTALL)
        
        # 2. BeautifulSoupìœ¼ë¡œ íŒŒì‹±
        soup = BeautifulSoup(text_content, 'html.parser')

        # 3. ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±° (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        for tag in soup(['script', 'style', 'meta', 'link', 'header', 'footer', 'nav', 'aside', 'form']):
            tag.decompose()

        # 4. í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì¤„ë°”ê¿ˆ ìœ ì§€)
        text = soup.get_text(separator='\n', strip=True)

        # 5. í—¤ë”(Header) ì •ë³´ ì œê±°
        #    'ê²Œì‹œê¸€ ë‚´ìš©' ë§ˆì»¤ëŠ” ë¶ˆì•ˆì •í•˜ë¯€ë¡œ ì œê±°
        header_end_patterns = [
            r'ì¡°íšŒìˆ˜\s+\d+',
            r'\.(xlsx|pdf|hwp|doc|docx|zip|jpg|png|jpeg|gif)(\s|\n|$)', # ì²¨ë¶€íŒŒì¼
        ]
        
        last_header_end_index = -1
        for pattern in header_end_patterns:
            matches = list(re.finditer(pattern, text, re.IGNORECASE))
            if matches:
                # ë§ˆì§€ë§‰ ì¼ì¹˜ í•­ëª©ì˜ ë ìœ„ì¹˜ë¥¼ ì°¾ìŒ
                last_match_end = matches[-1].end()
                if last_match_end > last_header_end_index:
                    last_header_end_index = last_match_end

        if last_header_end_index != -1 and last_header_end_index < len(text):
            text = text[last_header_end_index:] # í—¤ë” ë§ˆì»¤ *ì´í›„*ì˜ í…ìŠ¤íŠ¸

        # 6. í‘¸í„°(Footer) ì •ë³´ ì œê±°
        #    'ëª©ë¡' ê´€ë ¨ ë§ˆì»¤ëŠ” ë¶ˆì•ˆì •í•˜ë¯€ë¡œ ì œê±°
        footer_markers = [
            r'ì—°ì„¸ëŒ€í•™êµ ì˜ê³¼ëŒ€í•™ TAG',
            r'\sTAG\s',
            r'ì—°ì„¸ëŒ€í•™êµ ê´€ë ¨ì‚¬ì´íŠ¸',
            r'COPYRIGHTÂ©',
            r'ì±„ìš©ê³µê³ \s+ì…ì°°ê³µê³ ',
            r'ê°œì¸ì •ë³´ì²˜ë¦¬ë°©ì¹¨',
            # r'ëª©ë¡ ì´ì „ê¸€', # <-- ì´ ë§ˆì»¤ê°€ ë¬¸ì œë¥¼ ì¼ìœ¼í‚´ (ì œê±°)
        ]
        footer_pattern = re.compile('|'.join(footer_markers), re.IGNORECASE | re.DOTALL)
        match = footer_pattern.search(text)
        if match:
            text = text[:match.start()] # í‘¸í„° ë§ˆì»¤ *ì´ì „*ì˜ í…ìŠ¤íŠ¸

        # 7. 'ê²Œì‹œê¸€ ë‚´ìš©' í…ìŠ¤íŠ¸ê°€ ë‚¨ì•„ìˆë‹¤ë©´ ì§ì ‘ ì œê±°
        text = text.replace("ê²Œì‹œê¸€ ë‚´ìš©", "")
        
        # 8. ë§ˆì§€ë§‰ìœ¼ë¡œ 1ë‹¨ê³„ì—ì„œ ìˆ˜ì •í•œ clean_text í•¨ìˆ˜ë¡œ ìµœì¢… ì •ë¦¬
        return clean_text(text)
        
    except Exception as e:
        logger.warning(f"  âš ï¸ HTML parsing error: {e}")
        return "" # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜

def normalize_url(url: Optional[str], base_url: Optional[str] = None) -> str:
    """URL ì •ê·œí™” (ì ˆëŒ€ ê²½ë¡œ ë³€í™˜, fragment ì œê±° ë“±)"""
    if not url: return ""
    url = url.strip()

    # ìƒëŒ€ ê²½ë¡œ -> ì ˆëŒ€ ê²½ë¡œ ë³€í™˜
    if base_url and not url.startswith(('http://', 'https://', '//', 'javascript:')):
        try:
            url = urljoin(base_url, url)
        except ValueError:
             logger.warning(f"  âš ï¸ Could not join base_url '{base_url}' and relative url '{url}'")
             return "" # URL ê²°í•© ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´

    # í”„ë¡œí† ì½œ ì—†ëŠ” URL ì²˜ë¦¬ (ì˜ˆ: //example.com)
    if url.startswith('//'):
        url = 'https:' + url

    # javascript: ë§í¬ ë¬´ì‹œ
    if url.startswith('javascript:'):
        return ""

    # URL íŒŒì‹±í•˜ì—¬ ìœ íš¨ì„± ê²€ì‚¬ (scheme, netloc í™•ì¸)
    parsed = urlparse(url)
    if not parsed.scheme or not parsed.netloc:
        logger.debug(f"  âš ï¸ Invalid URL structure: {url}")
        return ""

    # Fragment ì œê±° (ì˜ˆ: #section1)
    url = url.split('#')[0]

    return url

def parse_dt(v: Any) -> Optional[datetime]:
    """ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë‚ ì§œ/ì‹œê°„ ê°’ì„ UTC datetime ê°ì²´ë¡œ íŒŒì‹±"""
    if not v: return None

    # ì´ë¯¸ datetime ê°ì²´ì¸ ê²½ìš°
    if isinstance(v, datetime):
        # íƒ€ì„ì¡´ ì •ë³´ê°€ ì—†ìœ¼ë©´ UTCë¡œ ê°„ì£¼
        return v if v.tzinfo else v.replace(tzinfo=timezone.utc)

    # íƒ€ì„ìŠ¤íƒ¬í”„ (ì´ˆ ë˜ëŠ” ë°€ë¦¬ì´ˆ)ì¸ ê²½ìš°
    if isinstance(v, (int, float)):
        try:
            # ë°€ë¦¬ì´ˆì¸ì§€ ì´ˆì¸ì§€ ì¶”ì •
            ts = v / 1000 if v > 10_000_000_000 else v
            return datetime.fromtimestamp(ts, tz=timezone.utc)
        except (ValueError, OSError): # ìœ íš¨í•˜ì§€ ì•Šì€ íƒ€ì„ìŠ¤íƒ¬í”„
            logger.warning(f"  âš ï¸ Invalid timestamp: {v}")
            return None

    # ë¬¸ìì—´ì¸ ê²½ìš°
    if isinstance(v, str):
        v = v.strip()
        if not v: return None

        # ISO 8601 í˜•ì‹ (+/-HH:MM ë˜ëŠ” Z í¬í•¨) ìš°ì„  ì²˜ë¦¬
        if re.search(r'[+-]\d{2}:\d{2}$|Z$', v):
            v = v.replace("Z", "+00:00") # Zë¥¼ UTC ì˜¤í”„ì…‹ìœ¼ë¡œ ë³€ê²½
            try:
                # íŒŒì‹± ì „ ë§ˆì´í¬ë¡œì´ˆ ë¶€ë¶„ ê¸¸ì´ ì¡°ì • (ìµœëŒ€ 6ìë¦¬)
                if '.' in v and ('+' in v or ('-' in v and v.rfind('-') > v.find('T'))):
                    parts = v.rsplit('+', 1) if '+' in v else v.rsplit('-', 1)
                    time_part = parts[0]
                    tz_part = parts[1]
                    if '.' in time_part:
                       time_part = time_part[:time_part.find('.')+7] # ë§ˆì´í¬ë¡œì´ˆ 6ìë¦¬ê¹Œì§€ë§Œ
                    v = f"{time_part}{'+' if '+' in v else '-'}{tz_part}"

                dt_obj = datetime.fromisoformat(v)
                return dt_obj.astimezone(timezone.utc) # UTCë¡œ ë³€í™˜
            except ValueError as e:
                logger.debug(f"  âš ï¸ ISO format parse error for '{v}': {e}")
                pass # ë‹¤ë¥¸ í˜•ì‹ ì‹œë„

        # ë‹¤ì–‘í•œ ì¼ë°˜ ë‚ ì§œ/ì‹œê°„ í˜•ì‹ ì‹œë„
        date_formats = [
            "%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M", "%Y-%m-%d",
            "%Y/%m/%d %H:%M:%S", "%Y/%m/%d %H:%M", "%Y/%m/%d",
            "%Y.%m.%d %H:%M:%S", "%Y.%m.%d %H:%M", "%Y.%m.%d", # ì  êµ¬ë¶„
            "%d/%m/%Y %H:%M:%S", "%d/%m/%Y", "%d-%m-%Y", # DD/MM/YYYY í˜•ì‹
            "%Yë…„ %mì›” %dì¼ %H:%M", "%Yë…„ %mì›” %dì¼", # í•œêµ­ì–´ í˜•ì‹
        ]
        for fmt in date_formats:
            try:
                dt_obj = datetime.strptime(v, fmt)
                # íƒ€ì„ì¡´ ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ UTCë¡œ ì„¤ì •
                return dt_obj.replace(tzinfo=timezone.utc)
            except ValueError:
                continue # ë‹¤ìŒ í˜•ì‹ ì‹œë„

    # ì–´ë–¤ í˜•ì‹ì—ë„ ë§ì§€ ì•Šìœ¼ë©´ None ë°˜í™˜
    logger.debug(f"  âš ï¸ Unparseable date format: {v} (type: {type(v)})")
    return None

def extract_field(item: Dict[str, Any], field_names: List[str], default: Any = "") -> Optional[Any]:
    """ì—¬ëŸ¬ í•„ë“œ ì´ë¦„ í›„ë³´ ì¤‘ ì²« ë²ˆì§¸ë¡œ ì°¾ì€ ê°’ì„ ë°˜í™˜ (ì  í‘œê¸°ë²• ì§€ì›)"""
    for field in field_names:
        if '.' in field: # ì¤‘ì²©ëœ í•„ë“œ ì ‘ê·¼ (ì˜ˆ: 'meta.title')
            parts = field.split('.')
            value = item
            valid_path = True
            for part in parts:
                if isinstance(value, dict):
                    value = value.get(part)
                elif isinstance(value, list) and part.isdigit() and int(part) < len(value):
                    # ë¦¬ìŠ¤íŠ¸ ì¸ë±ìŠ¤ ì ‘ê·¼ (ì œí•œì  ì§€ì›)
                    value = value[int(part)]
                else:
                    value = None
                    valid_path = False
                    break
            if valid_path and value is not None:
                return value
        else: # ì¼ë°˜ í•„ë“œ ì ‘ê·¼
            value = item.get(field)
            if value is not None:
                return value
    # ëª¨ë“  í›„ë³´ í•„ë“œì— ê°’ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ë°˜í™˜
    return default

# â­ï¸ [ìˆ˜ì • 2] normalize_item: raw_textë¥¼ ë°˜í™˜ ë”•ì…”ë„ˆë¦¬ì— ì¶”ê°€
def normalize_item(item: dict, base_url: Optional[str] = None, college_key: Optional[str] = None) -> dict:
    """Apify í¬ë¡¤ë§ ê²°ê³¼ itemì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”"""
    # ì œëª© ì¶”ì¶œ (ì—¬ëŸ¬ í•„ë“œëª… í›„ë³´ ì‚¬ìš©)
    title = clean_text(extract_field(item, ["title", "name", "subject", "headline", "meta.title", "og:title", "titleText", "h1", "h2"], default=""), max_length=500)
    # URL ì¶”ì¶œ ë° ì •ê·œí™”
    url = normalize_url(extract_field(item, ["url", "link", "href", "permalink", "canonical", "meta.url", "og:url"], default=""), base_url)
    # HTML ë³¸ë¬¸ ì¶”ì¶œ
    body_html = extract_field(item, ["html", "content_html", "body_html", "htmlContent", "content", "text"], default=None) # HTMLì€ ê·¸ëŒ€ë¡œ ìœ ì§€ ì‹œë„
    
    # ì›ë³¸ 'content'/'text' í•„ë“œ ì¶”ì¶œ (ì´ê²ƒì´ raw_textê°€ ë¨)
    raw_text = extract_field(item, ["text", "content", "body", "body_text", "plainText"], default=None)
    
    # raw_textë¥¼ ê¸°ë°˜ìœ¼ë¡œ body_text ì •ì œ
    body_text = clean_body_text(raw_text, college_key=college_key)

    # (ë¹„ìƒ ë¡œì§) ë§Œì•½ raw_textì— ë‚´ìš©ì´ ì—†ê³  body_htmlì—ë§Œ ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°,
    # body_htmlì„ ì •ì œ ì‹œë„
    if not body_text and body_html:
         # ì´ ê²½ìš°, ì›ë³¸ í…ìŠ¤íŠ¸ê°€ body_htmlì´ë¯€ë¡œ raw_textë„ body_htmlë¡œ ì„¤ì •
         if not raw_text: 
             raw_text = body_html
         body_text = clean_body_text(body_html, college_key=college_key)

    # ë°œí–‰ì¼ ì¶”ì¶œ (ì—¬ëŸ¬ í•„ë“œëª… í›„ë³´ ì‚¬ìš© ë° íŒŒì‹±)
    published_at = None
    date_fields = ["publishedAt", "published_at", "createdAt", "created_at", "datetime", "timestamp", "pubDate", "date", "time", "postDate", "releaseDate"]
    for field in date_fields:
        value = extract_field(item, [field], default=None) # ì¤‘ì²© í•„ë“œë„ ê°€ëŠ¥í•˜ê²Œ extract_field ì‚¬ìš©
        if value:
            parsed = parse_dt(value)
            if parsed:
                # ë„ˆë¬´ ì˜¤ë˜ëœ ë‚ ì§œëŠ” ì˜¤ë¥˜ ê°€ëŠ¥ì„± ìˆìœ¼ë¯€ë¡œ ë¡œê·¸ ë‚¨ê¸°ê³  ë¬´ì‹œ (ì˜ˆ: 1990ë…„ ì´ì „)
                if parsed.year >= 1990:
                    published_at = parsed
                    break # ì²« ë²ˆì§¸ ì„±ê³µí•œ íŒŒì‹± ê²°ê³¼ ì‚¬ìš©
                else:
                     logger.debug(f"  âš ï¸ Skipping date parse due to unlikely year: {parsed} from field '{field}'")

    # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜ (raw_text ì¶”ê°€)
    result = {
        "title": title,
        "url": url,
        "body_html": body_html,
        "raw_text": raw_text, # â­ï¸ ì›ë³¸ í…ìŠ¤íŠ¸(content)
        "body_text": body_text, # â­ï¸ ì •ì œëœ í…ìŠ¤íŠ¸
        "published_at": published_at, # íŒŒì‹±ëœ datetime ê°ì²´ ë˜ëŠ” None
    }
    return result

def validate_normalized_item(item: dict) -> bool:
    """ì •ê·œí™”ëœ itemì˜ í•„ìˆ˜ í•„ë“œ ë° ìœ íš¨ì„± ê²€ì¦"""
    # ì œëª©ê³¼ URLì€ í•„ìˆ˜
    if not item.get("title") or not item.get("url"):
        logger.warning(f"  âš ï¸ Skipping item due to missing title or URL: {item.get('url') or 'No URL'}")
        return False

    # URL í˜•ì‹ ê²€ì¦ (scheme, netloc)
    parsed_url = urlparse(item["url"])
    if not parsed_url.scheme or not parsed_url.netloc:
        logger.warning(f"  âš ï¸ Skipping item due to invalid URL: {item['url']}")
        return False

    # ë„ˆë¬´ ì§§ì€ ì œëª© ë¬´ì‹œ (ì˜¤ë¥˜ ê°€ëŠ¥ì„±)
    if len(item["title"]) < 3:
         logger.debug(f"  âš ï¸ Skipping item due to short title: {item['title']}")
         return False

    # ë°œí–‰ì¼ ìœ íš¨ì„± ê²€ì¦ (datetime ê°ì²´ì´ê³ , ë„ˆë¬´ ì˜¤ë˜ë˜ì§€ ì•ŠìŒ)
    if item.get("published_at"):
        pub_dt = item["published_at"]
        # datetime ê°ì²´ê°€ ì•„ë‹ˆë©´ ì‹¤íŒ¨
        if not isinstance(pub_dt, datetime):
             logger.warning(f"  âš ï¸ Skipping item due to invalid date type: {type(pub_dt)}")
             return False
        # íƒ€ì„ì¡´ ì •ë³´ ê°•ì œ (UTC)
        if pub_dt.tzinfo is None or pub_dt.tzinfo.utcoffset(pub_dt) is None:
             item["published_at"] = pub_dt.replace(tzinfo=timezone.utc) # ì›ë³¸ item ë”•ì…”ë„ˆë¦¬ ì§ì ‘ ìˆ˜ì •
        # ë„ˆë¬´ ì˜¤ë˜ëœ ë‚ ì§œ ë¬´ì‹œ
        if item["published_at"].year < 1990:
             logger.debug(f"  âš ï¸ Skipping item due to very old date: {item['published_at']}")
             return False

    # ëª¨ë“  ê²€ì¦ í†µê³¼
    return True

def content_hash(college_key: str, title: str, url: str, published_at: Optional[datetime]) -> str:
    """ê³µì§€ì‚¬í•­ ë‚´ìš© ê¸°ë°˜ ê³ ìœ  í•´ì‹œ ìƒì„± (ì¤‘ë³µ ë°©ì§€ìš©)"""
    url = url.rstrip('/') # URL ë '/' ì œê±°í•˜ì—¬ ì •ê·œí™”
    date_str = published_at.strftime('%Y-%m-%d') if published_at else "" # ë‚ ì§œ ë¶€ë¶„ë§Œ ì‚¬ìš© (ì‹œê°„ ì œì™¸)
    # ì œëª© ì •ê·œí™”: íŠ¹ì • ë¬¸ì ì œê±°, ì†Œë¬¸ì ë³€í™˜, ê³µë°± ì •ë¦¬
    title_normalized = re.sub(r'[\'\"\[\]''"""]', '', title) # ë”°ì˜´í‘œ, ëŒ€ê´„í˜¸ ì œê±°
    title_normalized = re.sub(r'\s+', ' ', title_normalized.lower().strip()) # ì†Œë¬¸ì, ê³µë°± ì •ë¦¬

    # í•´ì‹œ ìƒì„± ê¸°ì¤€ ë¬¸ìì—´ ì¡°í•©
    base = f"{college_key}|{title_normalized}|{url}|{date_str}"
    h = hashlib.sha256(base.encode('utf-8')).hexdigest()
    return h


# --- ê¸°ì¡´ Apify í—¬í¼ í•¨ìˆ˜ (get_latest_run_for_task, fetch_dataset_items) ---
def get_latest_run_for_task(task_id: str, timeout=60):
    """Apify Taskì˜ ê°€ì¥ ìµœê·¼ ì„±ê³µí•œ Run ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
    url = f"https://api.apify.com/v2/actor-tasks/{task_id}/runs"
    params = {"token": APIFY_TOKEN, "limit": 1, "desc": "true"} # ìµœì‹  1ê°œë§Œ
    try:
        resp = SESSION.get(url, params=params, timeout=timeout)
        resp.raise_for_status() # HTTP ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
        data = resp.json()
        runs = data.get("data", {}).get("items", [])
    except requests.RequestException as e:
        logger.error(f"  âŒ Error fetching runs for task {task_id}: {e}")
        return None
    except json.JSONDecodeError:
        logger.warning(f"  âš ï¸ Invalid JSON response for task {task_id} runs")
        return None

    if not runs:
        logger.warning(f"  âš ï¸ No recent run found for task {task_id}")
        return None

    latest_run = runs[0]
    status = latest_run.get("status", "UNKNOWN")
    # ì„±ê³µí•œ Runë§Œ ë°˜í™˜
    if status == "SUCCEEDED":
        return latest_run
    else:
        run_id = latest_run.get("id", "N/A")
        logger.warning(f"  âš ï¸ Latest run {run_id} for task {task_id} status: {status}. Skipping.")
        return None

def fetch_dataset_items(dataset_id: str, timeout=300):
    """Apify Datasetì˜ ëª¨ë“  ì•„ì´í…œ ê°€ì ¸ì˜¤ê¸° (í˜ì´ì§• ì²˜ë¦¬)"""
    url = f"https://api.apify.com/v2/datasets/{dataset_id}/items"
    params = {"token": APIFY_TOKEN, "format": "json", "clean": "true", "limit": 1000} # í•œ ë²ˆì— 1000ê°œì”©
    all_items = []
    offset = 0
    max_items_limit = 5000 # ìµœëŒ€ 5000ê°œ ì•„ì´í…œ ì œí•œ (ë„ˆë¬´ ë§ì€ ë°ì´í„° ë°©ì§€)

    while True:
        try:
            current_params = params.copy()
            current_params["offset"] = offset
            logger.info(f"  Fetching items... offset={offset}, limit={current_params.get('limit')}")

            resp = SESSION.get(url, params=current_params, timeout=timeout)
            resp.raise_for_status()
            items_data = resp.json() # JSON íŒŒì‹±

            # ì‘ë‹µ ë°ì´í„° í˜•ì‹ í™•ì¸ (ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ë”•ì…”ë„ˆë¦¬)
            current_batch = []
            if isinstance(items_data, list):
                current_batch = items_data
            elif isinstance(items_data, dict) and 'items' in items_data:
                # Apify ìµœì‹  APIëŠ” 'items' í‚¤ë¥¼ í¬í•¨í•  ìˆ˜ ìˆìŒ
                current_batch = items_data['items']
                if not isinstance(current_batch, list):
                    logger.warning(f"  âš ï¸ 'items' key found but not a list in dataset {dataset_id}")
                    break
            else:
                 logger.warning(f"  âš ï¸ Unexpected data format from dataset {dataset_id}: {type(items_data)}")
                 break

            if not current_batch:
                 logger.info("  No more items found in this batch.")
                 break # ë” ì´ìƒ ì•„ì´í…œì´ ì—†ìœ¼ë©´ ì¢…ë£Œ

            all_items.extend(current_batch)
            logger.info(f"  Fetched {len(current_batch)} items. Total now: {len(all_items)}")

            # ë‹¤ìŒ í˜ì´ì§• ì˜¤í”„ì…‹ ì„¤ì •
            offset += len(current_batch) # ì‹¤ì œ ê°€ì ¸ì˜¨ ê°œìˆ˜ë§Œí¼ ì¦ê°€

            # ìµœëŒ€ ì•„ì´í…œ ì œí•œ í™•ì¸
            if len(all_items) >= max_items_limit:
                 logger.warning(f"  âš ï¸ Reached max items limit ({max_items_limit}). Stopping fetch.")
                 break

            # API í˜¸ì¶œ ê°„ ì§§ì€ ì§€ì—° (Rate limit ë°©ì§€)
            time.sleep(0.5)

        except requests.RequestException as e:
            logger.error(f"  âš ï¸ Items fetch error for dataset {dataset_id} at offset {offset}: {e}")
            break # ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì‹œ ì¤‘ë‹¨
        except json.JSONDecodeError:
            logger.error(f"  âš ï¸ Items JSON decode error for dataset {dataset_id} at offset {offset}")
            break # JSON íŒŒì‹± ì˜¤ë¥˜ ì‹œ ì¤‘ë‹¨
        except Exception as e:
            logger.error(f"  âš ï¸ Unexpected error fetching items for dataset {dataset_id}: {e}")
            break # ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ì‹œ ì¤‘ë‹¨

    logger.info(f"  Total items fetched for dataset {dataset_id}: {len(all_items)}")
    return all_items


# --- ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (run) ---
def run(
    job_dataset_id: Optional[str] = None,
    job_task_id: Optional[str] = None,
    job_run_id: Optional[str] = None,
    job_finished_at: Optional[str] = None,
):
    total_upserted = 0
    total_skipped = 0
    total_ai_batches = 0 # AI ë°°ì¹˜ í˜¸ì¶œ íšŸìˆ˜

    print(f"ğŸ¤– AI_IN_PIPELINE: {AI_IN_PIPELINE} (Title-based Hashtag Classification - Batch API calls)")
    print(f"â±ï¸ AI_SLEEP_SEC (between batches): {AI_SLEEP_SEC}")
    print(f"ğŸ”¢ AI_BATCH_SIZE: {AI_BATCH_SIZE}")

    conn = None
    try:
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        conn = psycopg2.connect(DATABASE_URL)
        conn.autocommit = False # íŠ¸ëœì­ì…˜ ê´€ë¦¬ë¥¼ ìœ„í•´ autocommit ë¹„í™œì„±í™”

        # RealDictCursor ì‚¬ìš©: ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°›ìŒ
        with conn.cursor(cursor_factory=RealDictCursor) as cur:
            # colleges.pyì— ì •ì˜ëœ ê° ëŒ€í•™ë³„ë¡œ ì²˜ë¦¬
            if job_dataset_id and not job_task_id:
                logger.warning("  âš ï¸ Queue job provided dataset_id without actor_task_id. Processing all colleges.")

            matched_college = False

            for ck, meta in COLLEGES.items():
                college_name = meta.get("name", "Unknown College")
                task_id = meta.get("task_id")
                site = meta.get("url") # ëŒ€í•™ë³„ ê¸°ë³¸ URL

                # task_idê°€ ì—†ìœ¼ë©´ í•´ë‹¹ ëŒ€í•™ ê±´ë„ˆë›°ê¸°
                if not task_id:
                    logger.warning(f"Skipping college {college_name} ({ck}) due to missing task_id.")
                    continue

                if job_task_id and task_id != job_task_id:
                    continue

                print(f"\nğŸ” Processing college: {college_name} ({ck})")

                run_id = None
                ds_id = None
                finished_at_display = "unknown"

                if job_dataset_id and (not job_task_id or task_id == job_task_id):
                    matched_college = True
                    run_id = job_run_id
                    ds_id = job_dataset_id
                    finished_at_display = job_finished_at or "from queue"
                else:
                    # ê°€ì¥ ìµœê·¼ ì„±ê³µí•œ Apify Run ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                    run_data = get_latest_run_for_task(task_id)
                    if not run_data:
                        # ìµœê·¼ ì„±ê³µ Runì´ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                        continue

                    run_id = run_data.get("id")
                    ds_id = run_data.get("defaultDatasetId")
                    finished_at_str = run_data.get("finishedAt", "unknown time")

                    # Run ì™„ë£Œ ì‹œê°„ í‘œì‹œ (íŒŒì‹± ì‹œë„)
                    try:
                         finished_at_dt = datetime.fromisoformat(finished_at_str.replace("Z", "+00:00"))
                         finished_at_display = finished_at_dt.strftime('%Y-%m-%d %H:%M:%S %Z')
                    except:
                         finished_at_display = finished_at_str

                if not ds_id:
                    logger.error(f"  âŒ No datasetId available for task {task_id} / college {ck}")
                    continue

                logger.info(f"  ğŸ“… Using data from run {run_id} (Finished: {finished_at_display})")

                # ë°ì´í„°ì…‹ ì•„ì´í…œ ê°€ì ¸ì˜¤ê¸°
                items = fetch_dataset_items(ds_id)
                if not items:
                     logger.warning(f"  âš ï¸ No items fetched from dataset {ds_id}. Skipping college.")
                     continue
                logger.info(f"  ğŸ“¦ Total items retrieved: {len(items)}")

                college_upserted = 0
                college_skipped = 0
                ai_call_count_batch = 0 # í˜„ì¬ ëŒ€í•™ì˜ AI ë°°ì¹˜ í˜¸ì¶œ ìˆ˜
                processed_items_data = [] # ì •ê·œí™” ë° ìœ íš¨ì„± ê²€ì‚¬ í†µê³¼í•œ ì•„ì´í…œ ì €ì¥
                items_to_process_ai = [] # AI ì²˜ë¦¬ê°€ í•„ìš”í•œ ì•„ì´í…œ ì •ë³´ ì €ì¥

                # --- 1ë‹¨ê³„: ì•„ì´í…œ ì •ê·œí™”, ìœ íš¨ì„± ê²€ì‚¬, í•´ì‹œ ìƒì„± ë° AI ì²˜ë¦¬ ëŒ€ìƒ ì„ ë³„ ---
                logger.info("  Preprocessing items...")
                processed_hashes_in_run = set() # í˜„ì¬ Run ë‚´ì—ì„œ ì¤‘ë³µ í•´ì‹œ ë°©ì§€

                for item_index, rec in enumerate(items):
                    # ì•„ì´í…œ ì •ê·œí™” ì‹œë„ (ck ì „ë‹¬)
                    try:
                        norm = normalize_item(rec, base_url=site, college_key=ck)
                    except Exception as norm_err:
                        logger.error(f"  âŒ Error normalizing item {item_index+1}: {norm_err}")
                        college_skipped += 1
                        continue

                    # ìœ íš¨ì„± ê²€ì‚¬
                    if not validate_normalized_item(norm):
                        college_skipped += 1
                        continue

                    # ì½˜í…ì¸  í•´ì‹œ ìƒì„± ë° ì¤‘ë³µ í™•ì¸
                    try:
                        h = content_hash(ck, norm["title"], norm["url"], norm.get("published_at"))
                        # í˜„ì¬ Run ë‚´ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ëœ í•´ì‹œë©´ ê±´ë„ˆë›°ê¸°
                        if h in processed_hashes_in_run:
                             logger.debug(f"  âš ï¸ Skipping duplicate hash within this run: {norm['title'][:30]}...")
                             college_skipped += 1
                             continue
                        processed_hashes_in_run.add(h) # ì²˜ë¦¬ëœ í•´ì‹œë¡œ ì¶”ê°€

                        # DB ì €ì¥ì„ ìœ„í•´ í•„ìš”í•œ ì •ë³´ ì¶”ê°€
                        norm['hash'] = h
                        norm['college_key'] = ck
                        norm['source_site'] = site # source_site ì¶”ê°€
                        processed_items_data.append(norm)

                        # AI ì²˜ë¦¬ ëŒ€ìƒ ì„ ë³„ (AI_IN_PIPELINE í™œì„±í™” ë° ì œëª© ì¡´ì¬ ì‹œ)
                        if AI_IN_PIPELINE and norm.get("title", "").strip():
                            body_for_ai = norm.get("body_text") or norm.get("raw_text") or ""
                            body_for_ai = clean_text(body_for_ai, max_length=1200)
                            items_to_process_ai.append({
                                "id": h, # í•´ì‹œê°’ì„ AI ê²°ê³¼ ë§¤í•‘ìš© IDë¡œ ì‚¬ìš©
                                "title": norm["title"],
                                "college_name": college_name, # ë‹¨ê³¼ëŒ€ ì´ë¦„ë„ AI ì»¨í…ìŠ¤íŠ¸ë¡œ ì œê³µ
                                "body": body_for_ai,
                            })
                    except Exception as hash_err:
                        logger.error(f"  âŒ Error generating hash for item {item_index+1} ('{norm.get('title', 'N/A')[:30]}...'): {hash_err}")
                        college_skipped += 1
                        continue

                logger.info(f"  Preprocessing done. Valid items: {len(processed_items_data)}, AI targets: {len(items_to_process_ai)}")

                # --- 2ë‹¨ê³„: ì œëª© ê¸°ë°˜ í•´ì‹œíƒœê·¸ ë°°ì¹˜ ì²˜ë¦¬ ---
                ai_results_map = {} # { "hash_id": ["#íƒœê·¸1", "#íƒœê·¸2"], ... } í˜•íƒœë¡œ ê²°ê³¼ ì €ì¥
                if AI_IN_PIPELINE and items_to_process_ai:
                    logger.info(f"  Starting AI batch classification (Batch size: {AI_BATCH_SIZE})...")
                    num_batches = (len(items_to_process_ai) + AI_BATCH_SIZE - 1) // AI_BATCH_SIZE
                    total_ai_batches += num_batches # ì „ì²´ ë°°ì¹˜ ìˆ˜ ëˆ„ì 

                    for i in range(num_batches):
                        batch_start_index = i * AI_BATCH_SIZE
                        batch_end_index = batch_start_index + AI_BATCH_SIZE
                        current_batch_input = items_to_process_ai[batch_start_index:batch_end_index]

                        if not current_batch_input:
                            continue

                        logger.info(f"  Processing AI Batch {i+1}/{num_batches} ({len(current_batch_input)} items)...")
                        ai_call_count_batch += 1 # í˜„ì¬ ëŒ€í•™ ë°°ì¹˜ í˜¸ì¶œ ìˆ˜ ì¦ê°€

                        # --- API í˜¸ì¶œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨) ---
                        retry_count = 0
                        max_retries = AI_MAX_RETRIES # ì¬ì‹œë„ íšŸìˆ˜ (í™˜ê²½ë³€ìˆ˜ë¡œ ì¡°ì • ê°€ëŠ¥)
                        batch_success = False
                        while retry_count <= max_retries:
                            try:
                                # ì²« ë°°ì¹˜ê°€ ì•„ë‹ˆê±°ë‚˜ ì¬ì‹œë„ ì‹œ ì§€ì—°
                                if i > 0 or retry_count > 0:
                                    sleep_duration = AI_SLEEP_SEC * (retry_count + 1) # ì¬ì‹œë„ ì‹œ ë” ê¸¸ê²Œ ëŒ€ê¸°
                                    logger.debug(f"    Sleeping for {sleep_duration:.1f}s before AI call...")
                                    time.sleep(sleep_duration)

                                # ë°°ì¹˜ ë¶„ë¥˜ í•¨ìˆ˜ í˜¸ì¶œ
                                batch_result = classify_hashtags_from_title_batch(current_batch_input)
                                ai_results_map.update(batch_result) # ê²°ê³¼ ë§µì— ì¶”ê°€
                                batch_success = True
                                logger.info(f"  Batch {i+1} completed successfully.")
                                break # ì„±ê³µ ì‹œ ì¬ì‹œë„ ë£¨í”„ íƒˆì¶œ

                            except Exception as e:
                                # Rate limit ì˜¤ë¥˜ (HTTP 429) ì²˜ë¦¬
                                if "429" in str(e) or "rate limit" in str(e).lower():
                                    retry_count += 1
                                    if retry_count <= max_retries:
                                        wait_time = (2 ** retry_count) * 5 # Exponential backoff (5s, 10s, 20s)
                                        logger.warning(f"  âš ï¸ Rate limit on Batch {i+1}. Retrying in {wait_time}s... ({retry_count}/{max_retries})")
                                        time.sleep(wait_time)
                                    else:
                                         logger.error(f"  âŒ Max retries ({max_retries}) reached for Batch {i+1} due to rate limit. Skipping AI for this batch.")
                                         break # ìµœëŒ€ ì¬ì‹œë„ ë„ë‹¬ ì‹œ í¬ê¸°
                                else:
                                    # ê·¸ ì™¸ AI ì˜¤ë¥˜
                                    logger.error(f"  âŒ AI batch classification failed for Batch {i+1}: {e}. Skipping AI for this batch.")
                                    break # ë³µêµ¬ ë¶ˆê°€ëŠ¥ ì˜¤ë¥˜ ì‹œ í¬ê¸°

                        # --- ì¬ì‹œë„ ë¡œì§ ì¢…ë£Œ ---
                        if not batch_success:
                            # ë°°ì¹˜ ì²˜ë¦¬ì— ì‹¤íŒ¨í•œ ì•„ì´í…œë“¤ì— ëŒ€í•´ ë¹ˆ ë¦¬ìŠ¤íŠ¸([]) ê²°ê³¼ ì„¤ì • (DB ì˜¤ë¥˜ ë°©ì§€)
                            for item_info in current_batch_input:
                                if item_info['id'] not in ai_results_map:
                                    ai_results_map[item_info['id']] = []


                logger.info("  AI Step 1 finished.")

                # --- 3ë‹¨ê³„: AI í›„ì²˜ë¦¬ ê²°ê³¼ ë§¤í•‘ ì¤€ë¹„ ---
                category_map: Dict[str, Optional[str]] = {}
                structured_info_map: Dict[str, Dict[str, Any]] = {}
                detailed_hashtags_map: Dict[str, List[str]] = {}

                for norm_item in processed_items_data:
                    item_hash = norm_item.get("hash")
                    if not item_hash:
                        continue

                    hashtags_ai_raw = ai_results_map.get(item_hash, [])
                    if not isinstance(hashtags_ai_raw, list):
                        logger.warning(
                            f"  âš ï¸ Hashtags for {item_hash} is not a list ({type(hashtags_ai_raw)}), forcing to []."
                        )
                        hashtags_ai_raw = []

                    if hashtags_ai_raw == ["#ì¼ë°˜"]:
                        main_category = "#ì¼ë°˜"
                    elif hashtags_ai_raw:
                        main_category = hashtags_ai_raw[0]
                    else:
                        main_category = None

                    category_map[item_hash] = main_category
                    structured_info_map[item_hash] = {}
                    detailed_hashtags_map[item_hash] = []

                # --- 4ë‹¨ê³„: ìê²©ìš”ê±´ ì¶”ì¶œ ---
                if AI_IN_PIPELINE and processed_items_data:
                    logger.info("  AI Step 2 (qualification extraction) starting...")
                    step2_processed = 0

                    for idx, norm_item in enumerate(processed_items_data):
                        item_hash = norm_item.get("hash")
                        if not item_hash:
                            continue

                        title_for_ai = norm_item.get("title") or ""
                        body_for_ai = (
                            norm_item.get("body_text")
                            or norm_item.get("raw_text")
                            or ""
                        )

                        if not (title_for_ai.strip() or body_for_ai.strip()):
                            continue

                        main_category = category_map.get(item_hash) or "#ì¼ë°˜"

                        attempt = 0
                        extracted_info: Optional[Dict[str, Any]] = None
                        while attempt <= AI_MAX_RETRIES:
                            if attempt > 0:
                                wait_time = max(AI_STEP2_SLEEP_SEC, (2 ** attempt) * AI_STEP2_SLEEP_SEC)
                                logger.warning(
                                    f"    âš ï¸ Step 2 retry for item {item_hash[:8]}..., waiting {wait_time:.1f}s ({attempt}/{AI_MAX_RETRIES})"
                                )
                                if wait_time > 0:
                                    time.sleep(wait_time)
                            else:
                                if idx > 0 and AI_STEP2_SLEEP_SEC > 0:
                                    time.sleep(AI_STEP2_SLEEP_SEC)

                            try:
                                result = extract_structured_info(title_for_ai, body_for_ai, main_category)
                                if isinstance(result, dict) and "error" not in result:
                                    extracted_info = result
                                else:
                                    extracted_info = {}
                                break
                            except Exception as e:
                                if "429" in str(e) or "rate limit" in str(e).lower():
                                    attempt += 1
                                    continue
                                logger.error(
                                    f"    âŒ Step 2 extraction failed for item {item_hash[:8]}...: {e}"
                                )
                                extracted_info = {}
                                break

                        if extracted_info is not None:
                            structured_info_map[item_hash] = extracted_info
                        step2_processed += 1

                    logger.info(f"  AI Step 2 processed {step2_processed} items.")

                # --- 5ë‹¨ê³„: ì„¸ë¶€ í•´ì‹œíƒœê·¸ ì¶”ì¶œ ---
                if AI_IN_PIPELINE and processed_items_data:
                    logger.info("  AI Step 3 (detailed hashtags) starting...")
                    step3_processed = 0

                    for idx, norm_item in enumerate(processed_items_data):
                        item_hash = norm_item.get("hash")
                        if not item_hash:
                            continue

                        main_category = category_map.get(item_hash)
                        if not main_category or main_category == "#ì¼ë°˜":
                            continue

                        title_for_ai = norm_item.get("title") or ""
                        body_for_ai = (
                            norm_item.get("body_text")
                            or norm_item.get("raw_text")
                            or ""
                        )

                        if not (title_for_ai.strip() or body_for_ai.strip()):
                            continue

                        attempt = 0
                        detailed_result: List[str] = []
                        while attempt <= AI_MAX_RETRIES:
                            if attempt > 0:
                                wait_time = max(AI_STEP3_SLEEP_SEC, (2 ** attempt) * AI_STEP3_SLEEP_SEC)
                                logger.warning(
                                    f"    âš ï¸ Step 3 retry for item {item_hash[:8]}..., waiting {wait_time:.1f}s ({attempt}/{AI_MAX_RETRIES})"
                                )
                                if wait_time > 0:
                                    time.sleep(wait_time)
                            else:
                                if idx > 0 and AI_STEP3_SLEEP_SEC > 0:
                                    time.sleep(AI_STEP3_SLEEP_SEC)

                            try:
                                detailed_result = extract_detailed_hashtags(
                                    title_for_ai,
                                    body_for_ai,
                                    main_category,
                                ) or []
                                break
                            except Exception as e:
                                if "429" in str(e) or "rate limit" in str(e).lower():
                                    attempt += 1
                                    continue
                                logger.error(
                                    f"    âŒ Step 3 extraction failed for item {item_hash[:8]}...: {e}"
                                )
                                detailed_result = []
                                break

                        if detailed_result:
                            detailed_hashtags_map[item_hash] = detailed_result
                        step3_processed += 1

                    logger.info(f"  AI Step 3 processed {step3_processed} items.")

                # --- 6ë‹¨ê³„: DB ì €ì¥ ë£¨í”„ (ì˜¤ë¥˜ ìˆ˜ì • ë° ë¡œê¹… ê°•í™”) ---
                logger.info("  Upserting data into database...")
                for norm_item in processed_items_data:
                    item_hash = norm_item.get('hash')
                    # í•´ì‹œ ì—†ìœ¼ë©´ ì²˜ë¦¬ ë¶ˆê°€
                    if not item_hash:
                        logger.warning(f"  âš ï¸ Skipping item due to missing hash (should not happen): {norm_item.get('title', 'N/A')[:30]}...")
                        college_skipped += 1
                        continue

                    # AI ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° (ê¸°ë³¸ê°’ ì„¤ì • ê°•í™”)
                    hashtags_ai = ai_results_map.get(item_hash, []) # ê¸°ë³¸ê°’ ë¹ˆ ë¦¬ìŠ¤íŠ¸
                    if not isinstance(hashtags_ai, list):
                        logger.warning(f"  âš ï¸ Hashtags for {item_hash} is not a list ({type(hashtags_ai)}), forcing to []. AI Map: {ai_results_map.get(item_hash)}")
                        hashtags_ai = []

                    main_category = category_map.get(item_hash)

                    # ì¹´í…Œê³ ë¦¬ ì„¤ì • (í•´ì‹œíƒœê·¸ ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜)
                    category_ai = main_category if main_category and main_category != "#ì¼ë°˜" else None

                    # ì¼ì • í•„ë“œ (í˜„ì¬ëŠ” íŒŒì‹± ë¯¸ì ìš©)
                    start_at_ai = None
                    end_at_ai = None

                    # ìê²©ìš”ê±´/ì„¸ë¶€íƒœê·¸ ê²°ê³¼
                    qualification_ai = structured_info_map.get(item_hash, {})
                    if not isinstance(qualification_ai, dict):
                        qualification_ai = {}

                    detailed_hashtags = detailed_hashtags_map.get(item_hash, [])
                    if not isinstance(detailed_hashtags, list):
                        detailed_hashtags = []
                    detailed_hashtags_db = detailed_hashtags if detailed_hashtags else None

                    # â­ï¸ [ìˆ˜ì •] DB ì €ì¥ ì‹œë„: íŒŒë¼ë¯¸í„°ì—ì„œ search_vector ê´€ë ¨ ì œê±°
                    try:
                        cur.execute(UPSERT_SQL, {
                            "college_key": norm_item.get('college_key'), # college_key í™•ì¸
                            "title": norm_item.get("title"),
                            "url": norm_item.get("url"),
                            "body_html": norm_item.get("body_html"),
                            "body_text": norm_item.get("body_text"), # ì •ì œëœ í…ìŠ¤íŠ¸
                            "raw_text": norm_item.get("raw_text"), # â­ï¸ ì›ë³¸ í…ìŠ¤íŠ¸
                            "published_at": norm_item.get("published_at"),
                            "source_site": norm_item.get('source_site'), # source_site í™•ì¸
                            "content_hash": item_hash,
                            "category_ai": category_ai,
                            "start_at_ai": start_at_ai,
                            "end_at_ai": end_at_ai,
                            "qualification_ai": Json(qualification_ai), # Json() ì‚¬ìš© (ì´ì œ qualification_aiëŠ” dict)
                            "hashtags_ai": hashtags_ai, # ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸
                            "detailed_hashtags": detailed_hashtags_db,
                        })
                        # cur.rowcount > 0 ì´ë©´ ì‹¤ì œë¡œ INSERT ë˜ëŠ” UPDATE ë°œìƒ
                        # logger.debug(f"Upsert executed for hash {item_hash}. Row count: {cur.rowcount}")
                        college_upserted += 1 # ì‹¤í–‰ ìì²´ë¥¼ ì„±ê³µìœ¼ë¡œ ì¹´ìš´íŠ¸ (ON CONFLICT DO UPDATEë„ í¬í•¨)

                    except psycopg2.Error as db_err:
                        conn.rollback() # í˜„ì¬ ì•„ì´í…œ ë¡¤ë°± (íŠ¸ëœì­ì…˜ ìœ ì§€)
                        # ìƒì„¸í•œ DB ì˜¤ë¥˜ ë¡œê·¸ ì¶œë ¥
                        pgcode = getattr(db_err, 'pgcode', 'N/A')
                        pgerror = getattr(db_err, 'pgerror', str(db_err)).strip()
                        diag = getattr(db_err, 'diag', None)
                        diag_message = diag.message_detail if diag and hasattr(diag, 'message_detail') else pgerror

                        logger.error(f"  âŒ DB error upserting '{norm_item.get('title', 'N/A')[:30]}...' (Hash: {item_hash}):")
                        logger.error(f"     Code: {pgcode}, Detail: {diag_message}")
                        college_skipped += 1
                        # ì—¬ê¸°ì„œ continue ë˜ëŠ” break ê²°ì • ê°€ëŠ¥ (ì¼ë‹¨ ê³„ì† ì§„í–‰)
                    except Exception as general_err:
                         conn.rollback() # í˜„ì¬ ì•„ì´í…œ ë¡¤ë°±
                         logger.error(f"  âŒ Unexpected error during upsert for '{norm_item.get('title', 'N/A')[:30]}...': {general_err}")
                         college_skipped += 1
                         # ì—¬ê¸°ì„œ continue ë˜ëŠ” break ê²°ì • ê°€ëŠ¥ (ì¼ë‹¨ ê³„ì† ì§„í–‰)

                # --- DB ì €ì¥ ë£¨í”„ ì¢…ë£Œ ---

                # í•œ ëŒ€í•™ ì²˜ë¦¬ í›„ ì»¤ë°‹ (ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¡¤ë°±ë˜ì—ˆìœ¼ë¯€ë¡œ ì„±ê³µí•œ ê²ƒë§Œ ì»¤ë°‹ë¨)
                conn.commit()
                logger.info(f"  âœ… Finished {college_name}: Upserted attempts={college_upserted}, Skipped={college_skipped}, AI Batches={ai_call_count_batch}")

                # í ì‘ì—…ìœ¼ë¡œ ì‹¤í–‰ëœ ê²½ìš°, ëŒ€ìƒ ë‹¨ê³¼ëŒ€ ì²˜ë¦¬ í›„ ì¢…ë£Œ
                if job_task_id and task_id == job_task_id:
                    break

            if job_task_id and not matched_college:
                logger.warning(f"  âš ï¸ No college matched actor_task_id={job_task_id}.")

                total_upserted += college_upserted
                total_skipped += college_skipped

            # --- ëª¨ë“  ëŒ€í•™ ì²˜ë¦¬ ë£¨í”„ ì¢…ë£Œ ---

    except psycopg2.Error as db_conn_err:
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìì²´ì˜ ë¬¸ì œ
        logger.critical(f"\nâŒ Database connection error: {db_conn_err}")
        # ì´ ê²½ìš° ì¶”ê°€ ì²˜ë¦¬ê°€ ì–´ë ¤ìš°ë¯€ë¡œ ì¢…ë£Œ
    except KeyboardInterrupt:
        # ì‚¬ìš©ìê°€ Ctrl+C ë“±ìœ¼ë¡œ ì¤‘ë‹¨ ì‹œ
        logger.warning("\nğŸš« Operation cancelled by user.")
        if conn:
            conn.rollback() # ì§„í–‰ ì¤‘ì´ë˜ íŠ¸ëœì­ì…˜ ë¡¤ë°±
    except Exception as e:
        # ê·¸ ì™¸ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜
        logger.exception(f"\nâŒ An unexpected error occurred during the run: {e}") # ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ í¬í•¨
        if conn:
            conn.rollback() # ì§„í–‰ ì¤‘ì´ë˜ íŠ¸ëœì­N ë¡¤ë°±
    finally:
        # í•­ìƒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ
        if conn:
            conn.close()
            logger.info("Database connection closed.")

    # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    print(f"\nâœ¨ Script finished.")
    print(f"Total upsert attempts: {total_upserted}")
    print(f"Total skipped items: {total_skipped}")
    print(f"Total AI batch calls: {total_ai_batches}")

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹œì‘ì 
if __name__ == "__main__":
    run()